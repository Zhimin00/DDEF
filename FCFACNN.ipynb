{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def normalize_data(data):\n",
    "    data -= np.mean(data)\n",
    "    data /= np.max(np.abs(data))\n",
    "    return data\n",
    "\n",
    "def create_train_and_test_data(trainId, testId, labels, data):\n",
    "    test_data = normalize_data(data[testId,:,:,:]).astype(np.float32)\n",
    "    test_labels = labels[testId]\n",
    "    train_data = normalize_data(data[trainId,:,:,:]).astype(np.float32)\n",
    "    train_labels = labels[trainId]\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions,1)==np.argmax(labels,1)))/predictions.shape[0]\n",
    "def dense_to_one_hot(labels_dense, num_classes=10):\n",
    "    \"\"\"Convert class labels from scalars to one-hot vectors\"\"\"\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    return labels_one_hot\n",
    "def combine_data(data1,data2,num_channels=2):\n",
    "    data1 = data1.reshape((data1.shape[0],data1.shape[1],data1.shape[2],1))\n",
    "    data2 = data2.reshape((data2.shape[0],data2.shape[1],data2.shape[2],1))\n",
    "    combined_data = np.zeros((data1.shape[0],data1.shape[1],data1.shape[2],data1.shape[3]*num_channels))\n",
    "    combined_data[:,:,:,0] = normalize_data(data1[:,:,:,0])\n",
    "    combined_data[:,:,:,1] = normalize_data(data2[:,:,:,0])\n",
    "    return combined_data\n",
    "def randomize_data(dataset, labels):\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_dataset = dataset[permutation,:,:,:]\n",
    "    shuffled_labels = labels[permutation,:]\n",
    "    return shuffled_dataset, shuffled_labels\n",
    "\n",
    "numROI = 116\n",
    "num_channels = 2\n",
    "num_labels = 2\n",
    "image_size = numROI\n",
    "patch_size = image_size\n",
    "depth = 64\n",
    "num_hidden = 96\n",
    "keep_pr = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical = pd.read_csv('D:/mmdps/ER/clinical.csv')\n",
    "y = clinical['Y'].values\n",
    "y_hot = dense_to_one_hot(y, num_classes=2)\n",
    "data1 = np.load('D:/mmdps/ER/BCI_boldnet.npy')\n",
    "data2 = np.load('D:/mmdps/ER/BCI_dtinet.npy')\n",
    "combined_data = combine_data(data1, data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data1.reshape((data1.shape[0],data1.shape[1],data1.shape[2],1))\n",
    "data2 = data2.reshape((data2.shape[0],data2.shape[1],data2.shape[2],1))\n",
    "data1_norm = normalize_data(data1)\n",
    "data2_norm = normalize_data(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.isnan(data2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 10\n",
    "kf = KFold(n_splits=num_folds,shuffle=True,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = num_labels\n",
    "n_batches = len(y) // batch_size\n",
    "def KL(alpha):\n",
    "    beta=tf.constant(np.ones((1,K)),dtype=tf.float32)\n",
    "    S_alpha = tf.reduce_sum(alpha,axis=1,keepdims=True)\n",
    "    S_beta = tf.reduce_sum(beta,axis=1,keepdims=True)\n",
    "    lnB = tf.lgamma(S_alpha) - tf.reduce_sum(tf.lgamma(alpha),axis=1,keepdims=True)\n",
    "    lnB_uni = tf.reduce_sum(tf.lgamma(beta),axis=1,keepdims=True) - tf.lgamma(S_beta)\n",
    "    \n",
    "    dg0 = tf.digamma(S_alpha)\n",
    "    dg1 = tf.digamma(alpha)\n",
    "    \n",
    "    kl = tf.reduce_sum((alpha - beta)*(dg1-dg0),axis=1,keepdims=True) + lnB + lnB_uni\n",
    "    return kl\n",
    "\n",
    "def mse_loss(p, alpha, global_step, annealing_step): \n",
    "    S = tf.reduce_sum(alpha, axis=1, keepdims=True) \n",
    "    E = alpha - 1\n",
    "    m = alpha / S\n",
    "  \n",
    "    A = tf.reduce_sum((p-m)**2, axis=1, keepdims=True) \n",
    "    B = tf.reduce_sum(alpha*(S-alpha)/(S*S*(S+1)), axis=1, keepdims=True) \n",
    "    \n",
    "    annealing_coef = tf.minimum(1.0,tf.cast(global_step/annealing_step,tf.float32))\n",
    "    \n",
    "    alp = E*(1-p) + 1 \n",
    "    C =  annealing_coef * KL(alp)\n",
    "    return (A + B) + C\n",
    "def loss_EDL(func=tf.digamma):\n",
    "    def loss_func(p, alpha, global_step, annealing_step): \n",
    "        S = tf.reduce_sum(alpha, axis=1, keepdims=True) \n",
    "        E = alpha - 1\n",
    "    \n",
    "        A = tf.reduce_sum(p * (func(S) - func(alpha)), axis=1, keepdims=True)\n",
    "    \n",
    "        annealing_coef = tf.minimum(1.0, tf.cast(global_step/annealing_step,tf.float32))\n",
    "    \n",
    "        alp = E*(1-p) + 1 \n",
    "        B =  annealing_coef * KL(alp)\n",
    "    \n",
    "        return (A + B)\n",
    "    return loss_func"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Softmax_train_test(data, kf, y, name,learning_rate=0.0001, lmb = 0.001, num_steps = 4001, loss_function = None):\n",
    "    test_labs1=[]\n",
    "    test_preds1=[]\n",
    "    num_channels = data.shape[3]\n",
    "    for train_id, test_id in kf.split(y):\n",
    "        print(test_id)\n",
    "        train_data, train_labels, test_data, test_labels =  create_train_and_test_data(train_id, test_id, y_hot, data)\n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "        \n",
    "        #input data placeholders\n",
    "            tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "            tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "            \n",
    "            #test data is a constant\n",
    "            tf_test_dataset = tf.constant(test_data)\n",
    "            \n",
    "            #network weight variables: Xavier initialization for better convergence in deep layers\n",
    "            layer1_weights = tf.get_variable(\"layer1_weights\", shape=[1, patch_size, num_channels, depth],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            layer1_biases = tf.Variable(tf.constant(0.001, shape=[depth]))\n",
    "            layer2_weights = tf.get_variable(\"layer2_weights\", shape=[patch_size, 1, depth, 2*depth],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            layer2_biases = tf.Variable(tf.constant(0.001, shape=[2*depth]))\n",
    "            layer3_weights = tf.get_variable(\"layer3_weights\", shape=[2*depth, num_hidden],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            layer3_biases = tf.Variable(tf.constant(0.01, shape=[num_hidden]))\n",
    "            layer4_weights = tf.get_variable(\"layer4_weights\", shape=[num_hidden, num_labels],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            layer4_biases = tf.Variable(tf.constant(0.01, shape=[num_labels]))\n",
    "        \n",
    "        #convolutional network architecture\n",
    "            def model(data, keep_pr):\n",
    "                #first layer: line-by-line convolution with ReLU and dropout\n",
    "                conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='VALID')\n",
    "                hidden = tf.nn.dropout(tf.nn.relu(conv+layer1_biases),keep_pr)\n",
    "                #second layer: convolution by column with ReLU and dropout\n",
    "                conv = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding='VALID')\n",
    "                hidden = tf.nn.dropout(tf.nn.relu(conv+layer2_biases),keep_pr)\n",
    "                #third layer: fully connected hidden layer with dropout and ReLU\n",
    "                shape = hidden.get_shape().as_list()\n",
    "                reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "                hidden = tf.nn.dropout(tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases),keep_pr)\n",
    "                #fourth (output) layer: fully connected layer with logits as output\n",
    "                return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "        \n",
    "            #calculate loss-function (cross-entropy) in training\n",
    "            logits = model(tf_train_dataset,keep_pr)\n",
    "            loss = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits_v2(labels = tf_train_labels, logits = logits))\n",
    "            l2_loss = (tf.nn.l2_loss(layer3_weights)+tf.nn.l2_loss(layer4_weights)) * lmb\n",
    "            if loss_function == None:\n",
    "                total_loss = loss\n",
    "            elif loss_function == 'loss+l2_loss':\n",
    "                total_loss = loss+l2_loss\n",
    "            #optimizer definition\n",
    "            #learning_rate = 0.001\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.9, beta2=0.9, epsilon=1e-08).minimize(total_loss) \n",
    "            \n",
    "            #calculate predictions from training data\n",
    "            train_prediction = tf.nn.softmax(logits)\n",
    "            #calculate predictions from test data (keep_pr of dropout is 1!)\n",
    "            test_prediction = tf.nn.softmax(model(tf_test_dataset,1))\n",
    "            \n",
    "            # nuber of iterations\n",
    "            #num_steps = 4001\n",
    "        \n",
    "        #start TensorFlow session\n",
    "        with tf.Session(graph=graph) as session:\n",
    "            tf.global_variables_initializer().run()\n",
    "            print('Initialized')\n",
    "            for step in range(num_steps):\n",
    "                \n",
    "                offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "                \n",
    "                if (offset == 0 ): #if we seen all train data at least once, re-randomize the order of instances\n",
    "                    train_data, train_labels = randomize_data(train_data, train_labels)\n",
    "                \n",
    "                #create batch    \n",
    "                batch_data = train_data[offset:(offset + batch_size), :, :, :]\n",
    "                batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "                \n",
    "                #feed batch data to the placeholders\n",
    "                feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "                _, l, predictions = session.run(\n",
    "                [optimizer, total_loss, train_prediction], feed_dict=feed_dict)\n",
    "                if l < 0.0005 or l > 5:\n",
    "                    print('Minibatch loss at step %d: %f' % (step, l))\n",
    "                    print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "                    print('Converged')\n",
    "                    break\n",
    "                # at every 2000. step give some feedback on the progress\n",
    "                if (step % 4000 == 0):\n",
    "                    print('Minibatch loss at step %d: %f' % (step, l))\n",
    "                    print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            \n",
    "            \n",
    "            #evaluate the trained model on the test data in the given fold\n",
    "            test_pred=test_prediction.eval()\n",
    "            print('Test accuracy: %.1f%%' % accuracy(test_pred, test_labels))\n",
    "            \n",
    "            #save test predictions and labels of this fold to a list\n",
    "            test_labs1.append(test_labels)\n",
    "            test_preds1.append(test_pred)\n",
    "\n",
    "    l=test_labs1[0]\n",
    "    p=test_preds1[0]   \n",
    "    #iterate through the cross-validation folds    \n",
    "    for i in range(1,num_folds):\n",
    "        l=np.vstack((l,test_labs1[i]))\n",
    "        p=np.vstack((p,test_preds1[i]))\n",
    "\n",
    "    #calculate final accuracy    \n",
    "    print('ROI softmax Test accuracy: %s, %.2f%%' % (name, accuracy(p, l)))\n",
    "    np.savez(\"Softmax %s.npz\" % name, labels=l,predictions=p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15 16 19 26]\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.737728\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 3479: 0.000498\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 50.0%\n",
      "[ 4 12 27 37]\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.723169\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 3415: 0.000499\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 75.0%\n",
      "[ 6  9 25 39]\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.768618\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 3986: 0.000499\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 50.0%\n",
      "[ 8 13 31 34]\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.749170\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 4000: 0.000543\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 4066: 0.000497\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 75.0%\n",
      "[ 0 17 24 33]\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.748372\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 3632: 0.000496\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 75.0%\n",
      "[ 1  5 11 29]\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.758362\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 3853: 0.000498\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 75.0%\n",
      "[ 2 21 30 36]\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.725075\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 3856: 0.000499\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 100.0%\n",
      "[ 3 23 32 35]\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.752815\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 3589: 0.000498\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 75.0%\n",
      "[10 18 20 22]\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.740217\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 3686: 0.000499\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 50.0%\n",
      "[ 7 14 28 38]\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.733084\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 3851: 0.000499\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 50.0%\n",
      "ROI softmax Test accuracy: BOLD, 67.50%\n"
     ]
    }
   ],
   "source": [
    "Softmax_train_test(data1_norm, kf, y, name='BOLD',learning_rate=0.0001, lmb = 0.001, num_steps = 20001, loss_function = 'loss+l2_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15 16 19 26]\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.760889\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.002300\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.000673\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 9076: 0.000498\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 75.0%\n",
      "[ 4 12 27 37]\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.793267\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 4000: 0.002044\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.000574\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8326: 0.000500\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 50.0%\n",
      "[ 6  9 25 39]\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.756650\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.002075\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.000738\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8620: 0.000494\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 75.0%\n",
      "[ 8 13 31 34]\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.755615\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.002023\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.000630\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8625: 0.000500\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 50.0%\n",
      "[ 0 17 24 33]\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.739356\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.001994\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.000580\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8527: 0.000495\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 25.0%\n",
      "[ 1  5 11 29]\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.749906\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.001822\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.000532\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8105: 0.000500\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 25.0%\n",
      "[ 2 21 30 36]\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.723513\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 4000: 0.049507\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.000692\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8856: 0.000497\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 100.0%\n",
      "[ 3 23 32 35]\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.738530\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 4000: 0.001898\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.000558\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8143: 0.000497\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 50.0%\n",
      "[10 18 20 22]\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.754981\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.001971\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.000569\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8435: 0.000499\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 50.0%\n",
      "[ 7 14 28 38]\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.743145\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 4000: 0.001903\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 7804: 0.000500\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 75.0%\n",
      "ROI softmax Test accuracy: DTI, 57.50%\n"
     ]
    }
   ],
   "source": [
    "Softmax_train_test(data2_norm, kf, y, name='DTI',learning_rate=0.0001, lmb = 0.001, num_steps = 20001, loss_function = 'loss+l2_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15 16 19 26]\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.727613\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 3265: 0.000497\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 50.0%\n",
      "[ 4 12 27 37]\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.721472\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 2975: 0.000499\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 50.0%\n",
      "[ 6  9 25 39]\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.760638\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 3138: 0.000500\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 75.0%\n",
      "[ 8 13 31 34]\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.794373\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 3316: 0.000497\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 75.0%\n",
      "[ 0 17 24 33]\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.731595\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 3575: 0.000499\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 75.0%\n",
      "[ 1  5 11 29]\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.710293\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 3294: 0.000500\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 75.0%\n",
      "[ 2 21 30 36]\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.759522\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 3363: 0.000499\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 100.0%\n",
      "[ 3 23 32 35]\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.785341\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 3096: 0.000498\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 75.0%\n",
      "[10 18 20 22]\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.751618\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 3300: 0.000499\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 50.0%\n",
      "[ 7 14 28 38]\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.772330\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 3024: 0.000499\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 50.0%\n",
      "ROI softmax Test accuracy: BOLD+DTI, 67.50%\n"
     ]
    }
   ],
   "source": [
    "Softmax_train_test(combined_data, kf, y, name='BOLD+DTI',learning_rate=0.0001, lmb = 0.001, num_steps = 20001, loss_function = 'loss+l2_loss')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDL 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EDL1_train_test(data, kf, y, name,learning_rate=0.0001, lmb = 0.001, num_steps = 4001, loss_function = None):\n",
    "    test_labs1=[]\n",
    "    test_preds1=[]\n",
    "    num_channels = data.shape[3]\n",
    "    for train_id, test_id in kf.split(y):\n",
    "        train_data, train_labels, test_data, test_labels =  create_train_and_test_data(train_id, test_id, y_hot, data)\n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "        \n",
    "        #input data placeholders\n",
    "            tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "            tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "            global_step = tf.Variable(initial_value=0, name='global_step', trainable=False)\n",
    "            annealing_step = tf.placeholder(dtype=tf.int32)\n",
    "            #test data is a constant\n",
    "            tf_test_dataset = tf.constant(test_data)\n",
    "            \n",
    "            #network weight variables: Xavier initialization for better convergence in deep layers\n",
    "            layer1_weights = tf.get_variable(\"layer1_weights\", shape=[1, patch_size, num_channels, depth],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            layer1_biases = tf.Variable(tf.constant(0.001, shape=[depth]))\n",
    "            layer2_weights = tf.get_variable(\"layer2_weights\", shape=[patch_size, 1, depth, 2*depth],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            layer2_biases = tf.Variable(tf.constant(0.001, shape=[2*depth]))\n",
    "            layer3_weights = tf.get_variable(\"layer3_weights\", shape=[2*depth, num_hidden],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            layer3_biases = tf.Variable(tf.constant(0.01, shape=[num_hidden]))\n",
    "            layer4_weights = tf.get_variable(\"layer4_weights\", shape=[num_hidden, num_labels],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            layer4_biases = tf.Variable(tf.constant(0.01, shape=[num_labels]))\n",
    "            \n",
    "        #convolutional network architecture\n",
    "            def model(data, keep_pr):\n",
    "                #first layer: line-by-line convolution with ReLU and dropout\n",
    "                conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='VALID')\n",
    "                hidden = tf.nn.dropout(tf.nn.relu(conv+layer1_biases),keep_pr)\n",
    "                #second layer: convolution by column with ReLU and dropout\n",
    "                conv = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding='VALID')\n",
    "                hidden = tf.nn.dropout(tf.nn.relu(conv+layer2_biases),keep_pr)\n",
    "                #third layer: fully connected hidden layer with dropout and ReLU\n",
    "                shape = hidden.get_shape().as_list()\n",
    "                reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "                hidden = tf.nn.dropout(tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases),keep_pr)\n",
    "                #fourth (output) layer: fully connected layer with logits as output\n",
    "                return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "        \n",
    "            logits = model(tf_train_dataset,keep_pr)\n",
    "            def edl1(logits):\n",
    "                e = tf.nn.relu(logits)\n",
    "                b = tf.nn.softmax(logits)\n",
    "                E = tf.reduce_sum(e, axis=1, keepdims=True)\n",
    "                alpha = E * b+1\n",
    "                prob = alpha/tf.reduce_sum(alpha, 1, keepdims=True)   \n",
    "                return prob, alpha\n",
    "            prob, alpha = edl1(logits)\n",
    "            loss = tf.reduce_mean(mse_loss(tf_train_labels, alpha, global_step, annealing_step))\n",
    "            l2_loss = (tf.nn.l2_loss(layer3_weights)+tf.nn.l2_loss(layer4_weights)) * lmb\n",
    "            \n",
    "            if loss_function == None:\n",
    "                total_loss = loss\n",
    "            elif loss_function == 'loss+l2_loss':\n",
    "                total_loss = loss+l2_loss\n",
    "            #optimizer definition\n",
    "            #learning_rate = 0.001\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.9, beta2=0.9, epsilon=1e-08).minimize(total_loss) \n",
    "            \n",
    "            #calculate predictions from training data\n",
    "            train_prediction = prob\n",
    "            #calculate predictions from test data (keep_pr of dropout is 1!)\n",
    "            test_prediction, _ = edl1(model(tf_test_dataset,1))\n",
    "            \n",
    "            # nuber of iterations\n",
    "            #num_steps = 4001\n",
    "        \n",
    "        #start TensorFlow session\n",
    "        with tf.Session(graph=graph) as session:\n",
    "            tf.global_variables_initializer().run()\n",
    "            print('Initialized')\n",
    "            for step in range(num_steps):\n",
    "                \n",
    "                offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "                \n",
    "                if (offset == 0 ): #if we seen all train data at least once, re-randomize the order of instances\n",
    "                    train_data, train_labels = randomize_data(train_data, train_labels)\n",
    "                \n",
    "                #create batch    \n",
    "                batch_data = train_data[offset:(offset + batch_size), :, :, :]\n",
    "                batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "                \n",
    "                #feed batch data to the placeholders\n",
    "                feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, annealing_step : 10* n_batches }\n",
    "                _, l, predictions = session.run(\n",
    "                    [optimizer, total_loss, train_prediction], feed_dict=feed_dict)\n",
    "               \n",
    "                if l < 0.0005 or l > 5:\n",
    "                    print('Minibatch loss at step %d: %f' % (step, l))\n",
    "                    print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "                    print('Converged')\n",
    "                    break\n",
    "                # at every 2000. step give some feedback on the progress\n",
    "                if (step % 4000 == 0):\n",
    "                    print('Minibatch loss at step %d: %f' % (step, l))\n",
    "                    print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            \n",
    "            \n",
    "            #evaluate the trained model on the test data in the given fold\n",
    "            test_pred=test_prediction.eval()\n",
    "            print('Test accuracy: %.1f%%' % accuracy(test_pred, test_labels))\n",
    "            \n",
    "            #save test predictions and labels of this fold to a list\n",
    "            test_labs1.append(test_labels)\n",
    "            test_preds1.append(test_pred)\n",
    "\n",
    "    l=test_labs1[0]\n",
    "    p=test_preds1[0]   \n",
    "    #iterate through the cross-validation folds    \n",
    "    for i in range(1,num_folds):\n",
    "        l=np.vstack((l,test_labs1[i]))\n",
    "        p=np.vstack((p,test_preds1[i]))\n",
    "\n",
    "    #calculate final accuracy    \n",
    "    print('ROI EDL Test accuracy: %s, %.2f%%' % (name, accuracy(p, l)))\n",
    "    np.savez(\"EDL1 %s.npz\" % name, labels=l, predictions=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 0.721999\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 4000: 0.012493\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.005952\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.002925\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.014805\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.000964\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 50.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.720596\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.014247\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.004961\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.002374\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.001241\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.000745\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.722993\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.011730\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.005436\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.003284\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.001763\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.000873\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 50.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.723547\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 4000: 0.012498\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.006028\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.003292\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.001994\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.001156\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.719347\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.012250\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.005640\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.003526\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.001876\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.001131\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.722673\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.017372\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.006455\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.003213\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.002015\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.001110\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 25.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.717900\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 4000: 0.011506\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.004801\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.002438\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.001451\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.000781\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 100.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.723464\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.012831\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.006073\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.003683\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.002035\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.001272\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.721958\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.012163\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.004869\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.002824\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.001420\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.000752\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.721779\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.011120\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.004284\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.002175\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.001221\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.000732\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 50.0%\n",
      "ROI EDL Test accuracy: BOLD, 65.00%\n"
     ]
    }
   ],
   "source": [
    "EDL1_train_test(data1_norm, kf, y, name='BOLD',learning_rate=0.0001, lmb = 0.001, num_steps = 20001, loss_function = 'loss+l2_loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 0.723220\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 4000: 0.014929\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.007952\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.005627\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.004111\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.002336\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 50.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.722242\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.014048\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.006978\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.004450\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.002925\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.001976\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 50.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.722373\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 4000: 0.015224\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.006832\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.003590\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.001999\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.001991\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.718431\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 4000: 0.015542\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.010119\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.004674\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.003289\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.002174\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.719519\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.013742\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.006330\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.003763\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.002448\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.001589\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 50.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.718872\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.018872\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.008220\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.004809\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.003596\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.001983\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 25.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.721312\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 4000: 0.013811\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.007739\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.004499\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.071763\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.002168\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 100.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.719559\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 4000: 0.014365\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.006317\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.003602\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.002684\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.002891\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 50.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.722737\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 4000: 0.015295\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.005958\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.007238\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.002205\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.001872\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 50.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.720403\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 4000: 0.018713\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.007709\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.005331\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.003448\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.002362\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 50.0%\n",
      "ROI EDL Test accuracy: DTI, 57.50%\n"
     ]
    }
   ],
   "source": [
    "EDL1_train_test(data2_norm, kf, y, name='DTI',learning_rate=0.0001, lmb = 0.001, num_steps = 20001, loss_function = 'loss+l2_loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 0.714565\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.009615\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.003815\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.002048\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.001206\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.000699\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.722565\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 4000: 0.013803\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.005103\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.002211\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.001176\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.000707\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.720385\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.206117\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 8000: 0.005297\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.002697\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.001463\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.001097\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.715305\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 4000: 0.439830\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 8000: 0.004852\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.002333\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.001361\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.000786\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.719769\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.011306\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.005535\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.002406\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.001265\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.000840\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.719415\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 4000: 0.010759\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.005035\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.002464\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.001440\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.000899\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 25.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.717591\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 4000: 0.011058\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.004830\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.002655\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.001487\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.001003\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 100.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.720443\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.010868\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.004393\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.002180\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.001184\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.000720\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.722118\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 4000: 0.011557\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.005441\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.002759\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.001472\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.000956\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 50.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.713103\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 4000: 0.084893\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.003236\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.001559\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.000848\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.000621\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 50.0%\n",
      "ROI EDL Test accuracy: BOLD+DTI, 67.50%\n"
     ]
    }
   ],
   "source": [
    "EDL1_train_test(combined_data, kf, y, name='BOLD+DTI',learning_rate=0.0001, lmb = 0.001, num_steps = 20001, loss_function = 'loss+l2_loss')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDL2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EDL2_train_test(data, kf, y, name,learning_rate=0.0001, lmb = 0.001, num_steps = 4001, loss_function = None):\n",
    "    test_labs1=[]\n",
    "    test_preds1=[]\n",
    "    num_channels = data.shape[3]\n",
    "    for train_id, test_id in kf.split(y):\n",
    "        train_data, train_labels, test_data, test_labels =  create_train_and_test_data(train_id, test_id, y_hot, data)\n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "        \n",
    "        #input data placeholders\n",
    "            tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "            tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "            global_step = tf.Variable(initial_value=0, name='global_step', trainable=False)\n",
    "            annealing_step = tf.placeholder(dtype=tf.int32)\n",
    "            #test data is a constant\n",
    "            tf_test_dataset = tf.constant(test_data)\n",
    "            \n",
    "            #network weight variables: Xavier initialization for better convergence in deep layers\n",
    "            layer1_weights = tf.get_variable(\"layer1_weights\", shape=[1, patch_size, num_channels, depth],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            layer1_biases = tf.Variable(tf.constant(0.001, shape=[depth]))\n",
    "            layer2_weights = tf.get_variable(\"layer2_weights\", shape=[patch_size, 1, depth, 2*depth],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            layer2_biases = tf.Variable(tf.constant(0.001, shape=[2*depth]))\n",
    "            layer3_weights = tf.get_variable(\"layer3_weights\", shape=[2*depth, num_hidden],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            layer3_biases = tf.Variable(tf.constant(0.01, shape=[num_hidden]))\n",
    "            layer4_weights = tf.get_variable(\"layer4_weights\", shape=[num_hidden, num_labels],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            layer4_biases = tf.Variable(tf.constant(0.01, shape=[num_labels]))\n",
    "            evidence1_weight = tf.get_variable('evidence1_weight', shape = [1])\n",
    "            evidence2_weight = tf.get_variable('evidence2_weight', shape = [1])\n",
    "        #convolutional network architecture\n",
    "            def model(data, keep_pr):\n",
    "                #first layer: line-by-line convolution with ReLU and dropout\n",
    "                conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='VALID')\n",
    "                hidden = tf.nn.dropout(tf.nn.relu(conv+layer1_biases),keep_pr)\n",
    "                #second layer: convolution by column with ReLU and dropout\n",
    "                conv = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding='VALID')\n",
    "                hidden = tf.nn.dropout(tf.nn.relu(conv+layer2_biases),keep_pr)\n",
    "                #third layer: fully connected hidden layer with dropout and ReLU\n",
    "                shape = hidden.get_shape().as_list()\n",
    "                reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "                hidden = tf.nn.dropout(tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases),keep_pr)\n",
    "                #fourth (output) layer: fully connected layer with logits as output\n",
    "                return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "        \n",
    "            logits = model(tf_train_dataset,keep_pr)\n",
    "            def BMF(logits):\n",
    "                evidence = tf.nn.relu(logits)\n",
    "                alpha = evidence + 1\n",
    "                K = 2\n",
    "                u = K / tf.reduce_sum(alpha, axis=1, keepdims=True) #uncertainty\n",
    "                prob = alpha/tf.reduce_sum(alpha, 1, keepdims=True) \n",
    "                return prob, u, alpha\n",
    "            prob, u, alpha = BMF(logits)    \n",
    "            loss = tf.reduce_mean(mse_loss(tf_train_labels, alpha, global_step, annealing_step))\n",
    "            l2_loss = (tf.nn.l2_loss(layer3_weights)+tf.nn.l2_loss(layer4_weights)) * lmb\n",
    "            \n",
    "            if loss_function == None:\n",
    "                total_loss = loss\n",
    "            elif loss_function == 'loss+l2_loss':\n",
    "                total_loss = loss+l2_loss\n",
    "            #optimizer definition\n",
    "            #learning_rate = 0.001\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.9, beta2=0.9, epsilon=1e-08).minimize(total_loss) \n",
    "            \n",
    "            #calculate predictions from training data\n",
    "            train_prediction = prob\n",
    "            #calculate predictions from test data (keep_pr of dropout is 1!)\n",
    "            test_prediction, test_u, _ = BMF(model(tf_test_dataset,1))\n",
    "            \n",
    "            # nuber of iterations\n",
    "            #num_steps = 4001\n",
    "        \n",
    "        #start TensorFlow session\n",
    "        with tf.Session(graph=graph) as session:\n",
    "            tf.global_variables_initializer().run()\n",
    "            print('Initialized')\n",
    "            for step in range(num_steps):\n",
    "                \n",
    "                offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "                \n",
    "                if (offset == 0 ): #if we seen all train data at least once, re-randomize the order of instances\n",
    "                    train_data, train_labels = randomize_data(train_data, train_labels)\n",
    "                \n",
    "                #create batch    \n",
    "                batch_data = train_data[offset:(offset + batch_size), :, :, :]\n",
    "                batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "                \n",
    "                #feed batch data to the placeholders\n",
    "                feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, annealing_step : 10* n_batches }\n",
    "                _, l, predictions = session.run(\n",
    "                    [optimizer, total_loss, train_prediction], feed_dict=feed_dict)\n",
    "               \n",
    "                if l < 0.0005 or l > 5:\n",
    "                    print('Minibatch loss at step %d: %f' % (step, l))\n",
    "                    print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "                    print('Converged')\n",
    "                    break\n",
    "                # at every 2000. step give some feedback on the progress\n",
    "                if (step % 4000 == 0):\n",
    "                    print('Minibatch loss at step %d: %f' % (step, l))\n",
    "                    print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            \n",
    "            \n",
    "            #evaluate the trained model on the test data in the given fold\n",
    "            test_pred=test_prediction.eval()\n",
    "            print('Test accuracy: %.1f%%' % accuracy(test_pred, test_labels))\n",
    "            \n",
    "            #save test predictions and labels of this fold to a list\n",
    "            test_labs1.append(test_labels)\n",
    "            test_preds1.append(test_pred)\n",
    "\n",
    "    l=test_labs1[0]\n",
    "    p=test_preds1[0]   \n",
    "    #iterate through the cross-validation folds    \n",
    "    for i in range(1,num_folds):\n",
    "        l=np.vstack((l,test_labs1[i]))\n",
    "        p=np.vstack((p,test_preds1[i]))\n",
    "\n",
    "    #calculate final accuracy    \n",
    "    print('ROI EDL2 Test accuracy: %s, %.2f%%' % (name, accuracy(p, l)))\n",
    "    np.savez(\"EDL2 %s.npz\" % name, labels=l, predictions=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 0.729131\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.004727\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.001741\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.001212\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.000853\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.000771\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.739604\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 4000: 0.005581\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.016125\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.001016\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.000820\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.000636\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.749505\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.005016\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.001674\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.001223\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.000883\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.000692\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 50.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.720016\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.004956\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.001881\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.001166\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.002122\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.000722\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.708152\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.005325\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.002045\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.001225\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.000832\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.000845\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.708819\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 4000: 0.005248\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.001843\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.001196\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.000984\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.000757\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.714145\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.005364\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.001703\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.001151\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.002606\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.000712\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 100.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.717550\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.005377\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.001722\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.001208\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.000899\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.000851\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.723059\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 4000: 0.004344\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.001901\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.000953\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.000785\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.000750\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 25.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.716675\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 4000: 0.005388\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.002055\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.001464\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.000880\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.000740\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 50.0%\n",
      "ROI EDL2 Test accuracy: BOLD, 67.50%\n"
     ]
    }
   ],
   "source": [
    "EDL2_train_test(data1_norm, kf, y, name='BOLD',learning_rate=0.0001, lmb = 0.001, num_steps = 20001, loss_function = 'loss+l2_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 0.725335\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 4000: 0.013257\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.005916\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.003783\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.002570\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.002232\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 50.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.718126\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.012271\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.005307\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.014449\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.010413\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.001828\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 50.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.727666\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.012252\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.006081\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.003901\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.003143\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.002201\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.719171\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 4000: 0.012482\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.004717\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.003549\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.002244\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.002382\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.723142\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.012007\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.004863\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.003563\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.002524\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.001901\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 25.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.713597\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.013712\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.013136\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.003399\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.002627\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.002011\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 25.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.719471\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 4000: 0.013308\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.006361\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.003778\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.003012\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.002077\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 100.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.729355\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 4000: 0.014706\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.019550\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.002978\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.003445\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.002185\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 50.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.712436\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 4000: 0.011843\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.004882\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.003654\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.002953\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.002147\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.723532\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.012498\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.022833\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.003849\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.004172\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.003222\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 50.0%\n",
      "ROI EDL2 Test accuracy: DTI, 57.50%\n"
     ]
    }
   ],
   "source": [
    "EDL2_train_test(data2_norm, kf, y, name='DTI',learning_rate=0.0001, lmb = 0.001, num_steps = 20001, loss_function = 'loss+l2_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 0.737706\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.003943\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.001515\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.000722\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.000585\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16356: 0.000493\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.719328\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 4000: 0.003633\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.001174\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.000688\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 14653: 0.000498\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.716928\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.003720\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.001253\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.000974\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.000575\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16164: 0.000495\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 50.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.715212\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.004169\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.001432\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.000904\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.000712\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 17206: 0.000498\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.723593\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 4000: 0.004008\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.001477\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.000879\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.000642\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16484: 0.000497\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 100.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.759948\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 4000: 0.003648\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.001191\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.000822\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.000583\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16526: 0.000486\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 50.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.738277\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 4000: 0.003878\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.001488\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.000942\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.000708\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 18074: 0.000490\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 100.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.729873\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 4000: 0.003793\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.001098\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.000787\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 15707: 0.000494\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.723905\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 4000: 0.009805\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.001238\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.030449\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 15956: 0.000492\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 50.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.679928\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 4000: 0.003737\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.001260\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.000794\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 15997: 0.000490\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 75.0%\n",
      "ROI EDL2 Test accuracy: BOLD+DTI, 72.50%\n"
     ]
    }
   ],
   "source": [
    "EDL2_train_test(combined_data, kf, y, name='BOLD+DTI',learning_rate=0.0001, lmb = 0.001, num_steps = 20001, loss_function = 'loss+l2_loss')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDL1_fuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EDL1_fuse(data, kf, y, name,learning_rate=0.0001, lmb = 0.001, num_steps = 4001, loss_function = None):\n",
    "    test_labs1=[]\n",
    "    test_preds1=[]\n",
    "    num_channels = data.shape[3]\n",
    "    for train_id, test_id in kf.split(y):\n",
    "        train_data, train_labels, test_data, test_labels =  create_train_and_test_data(train_id, test_id, y_hot, data)\n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "        \n",
    "        #input data placeholders\n",
    "            tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "            tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "            annealing_step = tf.placeholder(dtype=tf.int32) \n",
    "            global_step = tf.Variable(initial_value=0, name='global_step', trainable=False)\n",
    "            \n",
    "            #test data is a constant\n",
    "            tf_test_dataset = tf.constant(test_data)\n",
    "            \n",
    "            #network weight variables: Xavier initialization for better convergence in deep layers\n",
    "            layer1_weights = tf.get_variable(\"layer1_weights\", shape=[1, patch_size, 1, depth],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            layer1_biases = tf.Variable(tf.constant(0.001, shape=[depth]))\n",
    "            layer2_weights = tf.get_variable(\"layer2_weights\", shape=[patch_size, 1, depth, 2*depth],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            layer2_biases = tf.Variable(tf.constant(0.001, shape=[2*depth]))\n",
    "            layer3_weights = tf.get_variable(\"layer3_weights\", shape=[2*depth, num_hidden],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            layer3_biases = tf.Variable(tf.constant(0.01, shape=[num_hidden]))\n",
    "            layer4_weights = tf.get_variable(\"layer4_weights\", shape=[num_hidden, num_labels],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            layer4_biases = tf.Variable(tf.constant(0.01, shape=[num_labels]))\n",
    "            evidence1_weight = tf.get_variable('evidence1_weight', shape = [1])\n",
    "            evidence2_weight = tf.get_variable('evidence2_weight', shape = [1])\n",
    "        #convolutional network architecture\n",
    "            def model(data, keep_pr):\n",
    "                #first layer: line-by-line convolution with ReLU and dropout\n",
    "                conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='VALID')\n",
    "                hidden = tf.nn.dropout(tf.nn.relu(conv+layer1_biases),keep_pr)\n",
    "                #second layer: convolution by column with ReLU and dropout\n",
    "                conv = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding='VALID')\n",
    "                hidden = tf.nn.dropout(tf.nn.relu(conv+layer2_biases),keep_pr)\n",
    "                #third layer: fully connected hidden layer with dropout and ReLU\n",
    "                shape = hidden.get_shape().as_list()\n",
    "                reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "                hidden = tf.nn.dropout(tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases),keep_pr)\n",
    "                #fourth (output) layer: fully connected layer with logits as output\n",
    "                return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "            train_shape = [tf_train_dataset.shape[0], tf_train_dataset.shape[1], tf_train_dataset.shape[2],1]\n",
    "            logits1 = model(tf.reshape(tf_train_dataset[:,:,:,0], train_shape),keep_pr)\n",
    "            logits2 = model(tf.reshape(tf_train_dataset[:,:,:,1], train_shape),keep_pr)\n",
    "            def edl1_combine(logits1, logits2):\n",
    "                e1 = tf.nn.relu(logits1)\n",
    "                b1 = tf.nn.softmax(logits1)\n",
    "                alpha1= e1\n",
    "                S1 = tf.reduce_sum(alpha1, axis=1, keepdims=True)\n",
    "\n",
    "                e2 = tf.nn.relu(logits2)\n",
    "                b2 = tf.nn.softmax(logits2)\n",
    "                alpha2= e2\n",
    "                S2 = tf.reduce_sum(alpha2, axis=1, keepdims=True)\n",
    "\n",
    "                S = (S1+S2)/2\n",
    "                b = b1*b2#evidence1_weight*b1*evidence2_weight*b2\n",
    "                kappa = tf.reduce_sum(b, axis=1,keepdims=True)\n",
    "                b_new = b/kappa\n",
    "                alpha_new = S*b_new+1\n",
    "                prob = alpha_new / tf.reduce_sum(alpha_new, axis=1,keepdims=True) \n",
    "                return prob, alpha_new\n",
    "            prob, alpha_new = edl1_combine(logits1=logits1,logits2=logits2)        \n",
    "            loss = tf.reduce_mean(mse_loss(tf_train_labels, alpha_new, global_step, annealing_step))\n",
    "            l2_loss = (tf.nn.l2_loss(layer3_weights)+tf.nn.l2_loss(layer4_weights)) * 0.001\n",
    "\n",
    "            if loss_function == None:\n",
    "                total_loss = loss\n",
    "            elif loss_function == 'loss+l2_loss':\n",
    "                total_loss = loss+l2_loss\n",
    "            #optimizer definition\n",
    "            #learning_rate = 0.001\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.9, beta2=0.9, epsilon=1e-08).minimize(total_loss) \n",
    "            \n",
    "            #calculate predictions from training data\n",
    "            train_prediction = prob\n",
    "            #calculate predictions from test data (keep_pr of dropout is 1!)\n",
    "            test_shape = [tf_test_dataset.shape[0], tf_test_dataset.shape[1], tf_test_dataset.shape[2],1]\n",
    "            test_prediction,_ = edl1_combine(model(tf.reshape(tf_test_dataset[:,:,:,0], test_shape),1),\n",
    "                                            model(tf.reshape(tf_test_dataset[:,:,:,1], test_shape),1))\n",
    "            \n",
    "            # nuber of iterations\n",
    "            #num_steps = 4001\n",
    "        \n",
    "        #start TensorFlow session\n",
    "        with tf.Session(graph=graph) as session:\n",
    "            tf.global_variables_initializer().run()\n",
    "            print('Initialized')\n",
    "            for step in range(num_steps):\n",
    "                \n",
    "                offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "                \n",
    "                if (offset == 0 ): #if we seen all train data at least once, re-randomize the order of instances\n",
    "                    train_data, train_labels = randomize_data(train_data, train_labels)\n",
    "                \n",
    "                #create batch    \n",
    "                batch_data = train_data[offset:(offset + batch_size), :, :, :]\n",
    "                batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "                \n",
    "                #feed batch data to the placeholders\n",
    "                feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, annealing_step : 10* n_batches }\n",
    "                _, l, predictions = session.run(\n",
    "                    [optimizer, total_loss, train_prediction], feed_dict=feed_dict)\n",
    "               \n",
    "                if l < 0.001 or l > 5:\n",
    "                    print('Minibatch loss at step %d: %f' % (step, l))\n",
    "                    print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "                    print('Converged')\n",
    "                    break\n",
    "                # at every 2000. step give some feedback on the progress\n",
    "                if (step % 4000 == 0):\n",
    "                    print('Minibatch loss at step %d: %f' % (step, l))\n",
    "                    print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "                \n",
    "            #evaluate the trained model on the test data in the given fold\n",
    "            test_pred=test_prediction.eval()\n",
    "            print('Test accuracy: %.1f%%' % accuracy(test_pred, test_labels))\n",
    "            \n",
    "            #save test predictions and labels of this fold to a list\n",
    "            test_labs1.append(test_labels)\n",
    "            test_preds1.append(test_pred)\n",
    "\n",
    "    l=test_labs1[0]\n",
    "    p=test_preds1[0]   \n",
    "    #iterate through the cross-validation folds    \n",
    "    for i in range(1,num_folds):\n",
    "        l=np.vstack((l,test_labs1[i]))\n",
    "        p=np.vstack((p,test_preds1[i]))\n",
    "\n",
    "    #calculate final accuracy    \n",
    "    print('ROI EDL1_fuse Test accuracy: %s, %.2f%%' % (name, accuracy(p, l)))\n",
    "    np.savez(\"EDL1_fuse %s.npz\" % name, labels=l,predictions=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 0.719367\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 4000: 0.012067\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.006270\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.003561\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.002274\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.001492\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.722537\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.011868\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.491873\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 12000: 0.003518\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.002339\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.001675\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.719168\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 4000: 0.013455\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.006561\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.004416\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.002620\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.001899\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.717870\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 4000: 0.012515\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.005852\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.003401\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.002291\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.001556\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.721577\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 4000: 0.013786\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.006853\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.003849\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.002450\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.001582\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.722283\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 4000: 0.012094\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.005926\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.003238\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.001947\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.001287\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 50.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.721679\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 4000: 0.011773\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.005843\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.003340\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.002070\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.001323\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 100.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.720977\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 4000: 0.012335\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.005818\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.003370\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.001834\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.001236\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.721049\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 4000: 0.011534\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.005576\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.003310\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.001930\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.001275\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 25.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.721177\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 4000: 0.015087\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.007002\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.004156\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.002375\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.001530\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "ROI EDL1_fuse Test accuracy: BOLD_combinedDTI, 70.00%\n"
     ]
    }
   ],
   "source": [
    "EDL1_fuse(combined_data, kf, y, name='BOLD_combinedDTI',learning_rate=0.0001, lmb = 0.001, num_steps = 20001, loss_function = 'loss+l2_loss')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDL2_fuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EDL2_fuse(data, kf, y, name,learning_rate=0.0001, lmb = 0.001, num_steps = 4001, loss_function = None):\n",
    "    test_labs1=[]\n",
    "    test_preds1=[]\n",
    "    num_channels = data.shape[3]\n",
    "    for train_id, test_id in kf.split(y):\n",
    "        train_data, train_labels, test_data, test_labels =  create_train_and_test_data(train_id, test_id, y_hot, data)\n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "        \n",
    "        #input data placeholders\n",
    "            tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "            tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "            global_step = tf.Variable(initial_value=0, name='global_step', trainable=False)\n",
    "            annealing_step = tf.placeholder(dtype=tf.int32)\n",
    "            #test data is a constant\n",
    "            tf_test_dataset = tf.constant(test_data)\n",
    "            \n",
    "            #network weight variables: Xavier initialization for better convergence in deep layers\n",
    "            layer1_weights = tf.get_variable(\"layer1_weights\", shape=[1, patch_size, 1, depth],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            layer1_biases = tf.Variable(tf.constant(0.001, shape=[depth]))\n",
    "            layer2_weights = tf.get_variable(\"layer2_weights\", shape=[patch_size, 1, depth, 2*depth],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            layer2_biases = tf.Variable(tf.constant(0.001, shape=[2*depth]))\n",
    "            layer3_weights = tf.get_variable(\"layer3_weights\", shape=[2*depth, num_hidden],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            layer3_biases = tf.Variable(tf.constant(0.01, shape=[num_hidden]))\n",
    "            layer4_weights = tf.get_variable(\"layer4_weights\", shape=[num_hidden, num_labels],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            layer4_biases = tf.Variable(tf.constant(0.01, shape=[num_labels]))\n",
    "            evidence1_weight = tf.get_variable('evidence1_weight', shape = [1])\n",
    "            evidence2_weight = tf.get_variable('evidence2_weight', shape = [1])\n",
    "\n",
    "        #convolutional network architecture\n",
    "            def model(data, keep_pr):\n",
    "                #first layer: line-by-line convolution with ReLU and dropout\n",
    "                conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='VALID')\n",
    "                hidden = tf.nn.dropout(tf.nn.relu(conv+layer1_biases),keep_pr)\n",
    "                #second layer: convolution by column with ReLU and dropout\n",
    "                conv = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding='VALID')\n",
    "                hidden = tf.nn.dropout(tf.nn.relu(conv+layer2_biases),keep_pr)\n",
    "                #third layer: fully connected hidden layer with dropout and ReLU\n",
    "                shape = hidden.get_shape().as_list()\n",
    "                reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "                hidden = tf.nn.dropout(tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases),keep_pr)\n",
    "                #fourth (output) layer: fully connected layer with logits as output\n",
    "                return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "            \n",
    "            train_shape = [tf_train_dataset.shape[0], tf_train_dataset.shape[1], tf_train_dataset.shape[2],1]\n",
    "            logits1 = model(tf.reshape(tf_train_dataset[:,:,:,0], train_shape),keep_pr)\n",
    "            logits2 = model(tf.reshape(tf_train_dataset[:,:,:,1], train_shape),keep_pr)\n",
    "            k=2\n",
    "            def edl2_combine(logits1, logits2):\n",
    "                e1 = tf.nn.relu(logits1)\n",
    "                alpha1 = e1 + 1\n",
    "                S1 = tf.reduce_sum(alpha1, axis=1, keepdims=True)\n",
    "                u1 = k/S1\n",
    "                b1 = e1/S1\n",
    "                e2 = tf.nn.relu(logits2)\n",
    "                alpha2 = e2 + 1\n",
    "                S2 = tf.reduce_sum(alpha2, axis=1, keepdims=True)\n",
    "                u2 = k/S2\n",
    "                b2 = e2/S2\n",
    "\n",
    "                b = b1*b2 + b1*u2 + b2*u1\n",
    "                u = u1*u2\n",
    "                kappa = tf.reduce_sum(b,axis=1,keepdims=True)+u\n",
    "                b_new = b/kappa\n",
    "                u_new = u/kappa\n",
    "                S = k/u_new\n",
    "                alpha_new = S*b_new+1\n",
    "                prob = alpha_new / tf.reduce_sum(alpha_new, axis=1, keepdims=True)\n",
    "                return prob, u, alpha_new\n",
    "\n",
    "            prob, u, alpha_new = edl2_combine(logits1, logits2)\n",
    "\n",
    "            loss = tf.reduce_mean(mse_loss(tf_train_labels, alpha_new, global_step, annealing_step))\n",
    "            l2_loss = (tf.nn.l2_loss(layer3_weights)+tf.nn.l2_loss(layer4_weights)) * lmb\n",
    "            \n",
    "            if loss_function == None:\n",
    "                total_loss = loss\n",
    "            elif loss_function == 'loss+l2_loss':\n",
    "                total_loss = loss+l2_loss\n",
    "            #optimizer definition\n",
    "            #learning_rate = 0.001\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.9, beta2=0.9, epsilon=1e-08).minimize(total_loss) \n",
    "            \n",
    "            #calculate predictions from training data\n",
    "            train_prediction = prob\n",
    "            #calculate predictions from test data (keep_pr of dropout is 1!)\n",
    "            test_shape = [tf_test_dataset.shape[0], tf_test_dataset.shape[1], tf_test_dataset.shape[2],1]\n",
    "            test_prediction, test_u, _ = edl2_combine(model(tf.reshape(tf_test_dataset[:,:,:,0], test_shape),1),\n",
    "                                                       model(tf.reshape(tf_test_dataset[:,:,:,1], test_shape),1))            \n",
    "            # nuber of iterations\n",
    "            #num_steps = 4001\n",
    "        \n",
    "        #start TensorFlow session\n",
    "        with tf.Session(graph=graph) as session:\n",
    "            tf.global_variables_initializer().run()\n",
    "            print('Initialized')\n",
    "            for step in range(num_steps):\n",
    "                \n",
    "                offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "                \n",
    "                if (offset == 0 ): #if we seen all train data at least once, re-randomize the order of instances\n",
    "                    train_data, train_labels = randomize_data(train_data, train_labels)\n",
    "                \n",
    "                #create batch    \n",
    "                batch_data = train_data[offset:(offset + batch_size), :, :, :]\n",
    "                batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "                \n",
    "                #feed batch data to the placeholders\n",
    "                feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, annealing_step : 10* n_batches }\n",
    "                _, l, predictions = session.run(\n",
    "                    [optimizer, total_loss, train_prediction], feed_dict=feed_dict)\n",
    "               \n",
    "                \n",
    "                # at every 2000. step give some feedback on the progress\n",
    "                if l < 0.001 or l > 5:\n",
    "                    print('Minibatch loss at step %d: %f' % (step, l))\n",
    "                    print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "                    print('Converged')\n",
    "                    break\n",
    "                if (step % 4000 == 0):\n",
    "                    print('Minibatch loss at step %d: %f' % (step, l))\n",
    "                    print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "\n",
    "            \n",
    "            #evaluate the trained model on the test data in the given fold\n",
    "            test_pred=test_prediction.eval()\n",
    "            print('Test accuracy: %.1f%%' % accuracy(test_pred, test_labels))\n",
    "            \n",
    "            #save test predictions and labels of this fold to a list\n",
    "            test_labs1.append(test_labels)\n",
    "            test_preds1.append(test_pred)\n",
    "\n",
    "    l=test_labs1[0]\n",
    "    p=test_preds1[0]   \n",
    "    #iterate through the cross-validation folds    \n",
    "    for i in range(1,num_folds):\n",
    "        l=np.vstack((l,test_labs1[i]))\n",
    "        p=np.vstack((p,test_preds1[i]))\n",
    "\n",
    "    #calculate final accuracy    \n",
    "    print('ROI EDL2_fuse Test accuracy: %s, %.2f%%' % (name, accuracy(p, l)))\n",
    "    np.savez(\"EDL2_fuse %s.npz\" % name, labels=l,predictions=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 0.716227\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 2822: 0.000989\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 25.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.720585\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 2630: 0.000985\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 50.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.723585\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 2771: 0.000999\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.716653\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 3091: 0.000996\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.694048\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 2846: 0.000999\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 100.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.695257\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 2904: 0.000998\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 100.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.699067\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 2812: 0.000994\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 100.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.714529\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 2894: 0.000987\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.704265\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 2781: 0.000999\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 50.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.722053\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 2905: 0.000996\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 50.0%\n",
      "ROI EDL2_fuse Test accuracy: BOLD_combinedDTI, 70.00%\n"
     ]
    }
   ],
   "source": [
    "EDL2_fuse(combined_data, kf, y, name='BOLD_combinedDTI',learning_rate=0.0001, lmb = 0.001, num_steps = 20001, loss_function = 'loss+l2_loss')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDL1+EDL2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EDL12(data, kf, y, name,learning_rate=0.0001, lmb = 0.001, num_steps = 4001, loss_function = None):\n",
    "    test_labs1=[]\n",
    "    test_preds1=[]\n",
    "    num_channels = data.shape[3]\n",
    "    for train_id, test_id in kf.split(y):\n",
    "        train_data, train_labels, test_data, test_labels =  create_train_and_test_data(train_id, test_id, y_hot, data)\n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "        \n",
    "        #input data placeholders\n",
    "            tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "            tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "            global_step = tf.Variable(initial_value=0, name='global_step', trainable=False)\n",
    "            annealing_step = tf.placeholder(dtype=tf.int32)\n",
    "            #test data is a constant\n",
    "            tf_test_dataset = tf.constant(test_data)\n",
    "            \n",
    "            #network weight variables: Xavier initialization for better convergence in deep layers\n",
    "            layer1_weights = tf.get_variable(\"layer1_weights\", shape=[1, patch_size, num_channels, depth],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            layer1_biases = tf.Variable(tf.constant(0.001, shape=[depth]))\n",
    "            layer2_weights = tf.get_variable(\"layer2_weights\", shape=[patch_size, 1, depth, 2*depth],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            layer2_biases = tf.Variable(tf.constant(0.001, shape=[2*depth]))\n",
    "            layer3_weights = tf.get_variable(\"layer3_weights\", shape=[2*depth, num_hidden],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            layer3_biases = tf.Variable(tf.constant(0.01, shape=[num_hidden]))\n",
    "            layer4_weights = tf.get_variable(\"layer4_weights\", shape=[num_hidden, num_labels],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            layer4_biases = tf.Variable(tf.constant(0.01, shape=[num_labels]))\n",
    "            \n",
    "        #convolutional network architecture\n",
    "            def model(data, keep_pr):\n",
    "                #first layer: line-by-line convolution with ReLU and dropout\n",
    "                conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='VALID')\n",
    "                hidden = tf.nn.dropout(tf.nn.relu(conv+layer1_biases),keep_pr)\n",
    "                #second layer: convolution by column with ReLU and dropout\n",
    "                conv = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding='VALID')\n",
    "                hidden = tf.nn.dropout(tf.nn.relu(conv+layer2_biases),keep_pr)\n",
    "                #third layer: fully connected hidden layer with dropout and ReLU\n",
    "                shape = hidden.get_shape().as_list()\n",
    "                reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "                hidden = tf.nn.dropout(tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases),keep_pr)\n",
    "                #fourth (output) layer: fully connected layer with logits as output\n",
    "                return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "        \n",
    "            logits = model(tf_train_dataset,keep_pr)\n",
    "            \n",
    "            def edl12(logits):\n",
    "                e = tf.nn.relu(logits)\n",
    "                b2 = tf.nn.softmax(logits)\n",
    "\n",
    "                alpha1 = e + 1\n",
    "                S = tf.reduce_sum(alpha1,axis=1,keepdims=True)\n",
    "                b1 = e / S\n",
    "                u1 = num_labels/S\n",
    "                b = b1*b2+u1*b2\n",
    "\n",
    "                kappa = tf.reduce_sum(b,axis=1,keepdims=True)\n",
    "                b_new = (1-u1)*b/kappa\n",
    "                alpha_new = S*b_new+1\n",
    "                prob = alpha_new/tf.reduce_sum(alpha_new, 1, keepdims=True)   \n",
    "                return prob, alpha_new\n",
    "            prob, alpha = edl12(logits)\n",
    "            loss = tf.reduce_mean(mse_loss(tf_train_labels, alpha, global_step, annealing_step))\n",
    "            l2_loss = (tf.nn.l2_loss(layer3_weights)+tf.nn.l2_loss(layer4_weights)) * lmb\n",
    "            \n",
    "            if loss_function == None:\n",
    "                total_loss = loss\n",
    "            elif loss_function == 'loss+l2_loss':\n",
    "                total_loss = loss+l2_loss\n",
    "            #optimizer definition\n",
    "            #learning_rate = 0.001\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.9, beta2=0.9, epsilon=1e-08).minimize(total_loss) \n",
    "            \n",
    "            #calculate predictions from training data\n",
    "            train_prediction = prob\n",
    "            #calculate predictions from test data (keep_pr of dropout is 1!)\n",
    "            test_prediction, _ = edl12(model(tf_test_dataset,1))\n",
    "            \n",
    "            # nuber of iterations\n",
    "            #num_steps = 4001\n",
    "        \n",
    "        #start TensorFlow session\n",
    "        with tf.Session(graph=graph) as session:\n",
    "            tf.global_variables_initializer().run()\n",
    "            print('Initialized')\n",
    "            for step in range(num_steps):\n",
    "                \n",
    "                offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "                \n",
    "                if (offset == 0 ): #if we seen all train data at least once, re-randomize the order of instances\n",
    "                    train_data, train_labels = randomize_data(train_data, train_labels)\n",
    "                \n",
    "                #create batch    \n",
    "                batch_data = train_data[offset:(offset + batch_size), :, :, :]\n",
    "                batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "                \n",
    "                #feed batch data to the placeholders\n",
    "                feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, annealing_step : 10* n_batches }\n",
    "                _, l, predictions = session.run(\n",
    "                    [optimizer, total_loss, train_prediction], feed_dict=feed_dict)\n",
    "               \n",
    "                if l < 0.0005 or l > 5:\n",
    "                    print('Minibatch loss at step %d: %f' % (step, l))\n",
    "                    print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "                    print('Converged')\n",
    "                    break\n",
    "                # at every 2000. step give some feedback on the progress\n",
    "                if (step % 4000 == 0):\n",
    "                    print('Minibatch loss at step %d: %f' % (step, l))\n",
    "                    print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            \n",
    "            \n",
    "            #evaluate the trained model on the test data in the given fold\n",
    "            test_pred=test_prediction.eval()\n",
    "            print('Test accuracy: %.1f%%' % accuracy(test_pred, test_labels))\n",
    "            \n",
    "            #save test predictions and labels of this fold to a list\n",
    "            test_labs1.append(test_labels)\n",
    "            test_preds1.append(test_pred)\n",
    "\n",
    "    l=test_labs1[0]\n",
    "    p=test_preds1[0]   \n",
    "    #iterate through the cross-validation folds    \n",
    "    for i in range(1,num_folds):\n",
    "        l=np.vstack((l,test_labs1[i]))\n",
    "        p=np.vstack((p,test_preds1[i]))\n",
    "\n",
    "    #calculate final accuracy    \n",
    "    print('ROI EDL12 Test accuracy: %s, %.2f%%' % (name, accuracy(p, l)))\n",
    "    np.savez(\"EDL12 %s.npz\" % name, labels=l, predictions=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 0.719861\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 4000: 0.011975\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.004807\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.002526\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.001268\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.001883\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.717443\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 4000: 0.456241\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 8000: 0.004652\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.002393\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.001351\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.000794\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.720640\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.010259\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.004389\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.002153\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.001325\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.000786\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.723087\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 4000: 0.012641\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.005222\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.002976\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.001342\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.000798\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.719605\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 4000: 0.015176\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.005885\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.003208\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.002065\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.001100\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 100.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.721341\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 4000: 0.011628\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.005347\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.002645\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.001504\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.000996\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 25.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.721569\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 4000: 0.014306\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.006452\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.003527\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.001916\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.001023\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 100.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.721930\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 4000: 0.010612\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.310860\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 12000: 0.002673\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.001687\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.000993\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.718177\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 4000: 0.010098\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.004852\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.002565\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.001413\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.000843\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.722834\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 4000: 0.010771\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.005783\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.002514\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.017035\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.000856\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "ROI EDL12 Test accuracy: BOLD, 75.00%\n"
     ]
    }
   ],
   "source": [
    "EDL12(data1_norm, kf, y, name='BOLD',learning_rate=0.0001, lmb = 0.001, num_steps = 20001, loss_function = 'loss+l2_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 0.721094\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.017496\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.009018\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.004869\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.003353\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.006728\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.721438\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 4000: 0.018006\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.007347\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.004511\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.002800\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.001845\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 50.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.721187\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 4000: 0.014740\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.007232\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.004287\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.002813\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.002076\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.722602\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 4000: 0.015614\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.007089\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.005149\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.002966\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.002136\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 50.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.720423\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.015911\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.091788\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.005071\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.003247\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.002028\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 50.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.721487\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 4000: 0.015277\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.007545\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.004473\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.003497\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.001972\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 25.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.717340\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.018322\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.007739\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.006210\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.002551\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.002193\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 100.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.722454\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.016915\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.008766\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.006035\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.003762\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.004100\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 50.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.719049\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 4000: 0.015936\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.007944\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.004883\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.006349\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.002005\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 100.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.719529\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.016292\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.008236\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.004651\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.003196\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.002281\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 50.0%\n",
      "ROI EDL12 Test accuracy: DTI, 62.50%\n"
     ]
    }
   ],
   "source": [
    "EDL12(data2_norm, kf, y, name='DTI',learning_rate=0.0001, lmb = 0.001, num_steps = 20001, loss_function = 'loss+l2_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 0.723144\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 4000: 0.011353\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.004878\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.002458\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.001314\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.000785\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.723136\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 4000: 0.009467\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.003321\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.001949\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.000888\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.000583\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.714270\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.016947\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.004348\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.002137\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.001250\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.000695\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 50.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.716048\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 4000: 0.010511\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.004178\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.002959\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.000989\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.000736\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.721664\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.010346\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.295364\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 12000: 0.001998\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.001195\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.000712\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.719773\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 4000: 0.010174\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.004073\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.001967\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.001064\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.000646\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.721006\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.011374\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.004591\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.002337\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.001307\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.000754\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 100.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.722115\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 4000: 0.011305\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.004099\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.002172\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.001081\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.000733\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.721171\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.026438\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.005048\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.002528\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.001376\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.000759\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.713585\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.011793\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.004777\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 12000: 0.002354\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 16000: 0.001177\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 20000: 0.000717\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 50.0%\n",
      "ROI EDL12 Test accuracy: BOLD+DTI, 72.50%\n"
     ]
    }
   ],
   "source": [
    "EDL12(combined_data, kf, y, name='BOLD+DTI',learning_rate=0.0001, lmb = 0.001, num_steps = 20001, loss_function = 'loss+l2_loss')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDL1+EDL2_fuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EDL12_fuse(data, kf, y, name,learning_rate=0.0001, lmb = 0.001, num_steps = 4001, loss_function = None):\n",
    "    test_labs1=[]\n",
    "    test_preds1=[]\n",
    "    num_channels = data.shape[3]\n",
    "    for train_id, test_id in kf.split(y):\n",
    "        train_data, train_labels, test_data, test_labels =  create_train_and_test_data(train_id, test_id, y_hot, data)\n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "        \n",
    "        #input data placeholders\n",
    "            tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "            tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "            global_step = tf.Variable(initial_value=0, name='global_step', trainable=False)\n",
    "            annealing_step = tf.placeholder(dtype=tf.int32)\n",
    "            #test data is a constant\n",
    "            tf_test_dataset = tf.constant(test_data)\n",
    "            \n",
    "            #network weight variables: Xavier initialization for better convergence in deep layers\n",
    "            layer1_weights = tf.get_variable(\"layer1_weights\", shape=[1, patch_size, 1, depth],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            layer1_biases = tf.Variable(tf.constant(0.001, shape=[depth]))\n",
    "            layer2_weights = tf.get_variable(\"layer2_weights\", shape=[patch_size, 1, depth, 2*depth],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            layer2_biases = tf.Variable(tf.constant(0.001, shape=[2*depth]))\n",
    "            layer3_weights = tf.get_variable(\"layer3_weights\", shape=[2*depth, num_hidden],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            layer3_biases = tf.Variable(tf.constant(0.01, shape=[num_hidden]))\n",
    "            layer4_weights = tf.get_variable(\"layer4_weights\", shape=[num_hidden, num_labels],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            layer4_biases = tf.Variable(tf.constant(0.01, shape=[num_labels]))\n",
    "            evidence1_weight = tf.get_variable('evidence1_weight', shape = [1])\n",
    "            evidence2_weight = tf.get_variable('evidence2_weight', shape = [1])\n",
    "\n",
    "        #convolutional network architecture\n",
    "            def model(data, keep_pr):\n",
    "                #first layer: line-by-line convolution with ReLU and dropout\n",
    "                conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='VALID')\n",
    "                hidden = tf.nn.dropout(tf.nn.relu(conv+layer1_biases),keep_pr)\n",
    "                #second layer: convolution by column with ReLU and dropout\n",
    "                conv = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding='VALID')\n",
    "                hidden = tf.nn.dropout(tf.nn.relu(conv+layer2_biases),keep_pr)\n",
    "                #third layer: fully connected hidden layer with dropout and ReLU\n",
    "                shape = hidden.get_shape().as_list()\n",
    "                reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "                hidden = tf.nn.dropout(tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases),keep_pr)\n",
    "                #fourth (output) layer: fully connected layer with logits as output\n",
    "                return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "            \n",
    "            train_shape = [tf_train_dataset.shape[0], tf_train_dataset.shape[1], tf_train_dataset.shape[2],1]\n",
    "            logits1 = model(tf.reshape(tf_train_dataset[:,:,:,0], train_shape),keep_pr)\n",
    "            logits2 = model(tf.reshape(tf_train_dataset[:,:,:,1], train_shape),keep_pr)\n",
    "            def edl12_combine(logits1, logits2):\n",
    "                e1 = tf.nn.relu(logits1)\n",
    "                b12 = tf.nn.softmax(logits1)\n",
    "                alpha11 = e1 + 1\n",
    "                S1 = tf.reduce_sum(alpha11,axis=1,keepdims=True)\n",
    "                b11 = e1 / S1\n",
    "                u1 = num_labels/S1\n",
    "                b1 = b11*b12+u1*b12\n",
    "                kappa1 = tf.reduce_sum(b1,axis=1,keepdims=True)\n",
    "                b_new1 = (1-u1)*b1/kappa1\n",
    "\n",
    "                e2 = tf.nn.relu(logits2)\n",
    "                b22 = tf.nn.softmax(logits1)\n",
    "                alpha21 = e2 + 1\n",
    "                S2 = tf.reduce_sum(alpha21,axis=1,keepdims=True)\n",
    "                b21 = e2 / S2\n",
    "                u2 = num_labels/S2\n",
    "                b2 = b21*b22+u2*b22\n",
    "                kappa2 = tf.reduce_sum(b2,axis=1,keepdims=True)\n",
    "                b_new2 = (1-u2)*b2/kappa2\n",
    "\n",
    "                b = b_new1*b_new2 + b_new1*u2 + b_new2*u1\n",
    "                u = u1*u2\n",
    "                kappa = tf.reduce_sum(b,axis=1,keepdims=True)+u\n",
    "                b_new = b/kappa\n",
    "                u_new = u/kappa\n",
    "                S = num_labels/u_new\n",
    "                alpha_new = S*b_new+1\n",
    "                prob = alpha_new / tf.reduce_sum(alpha_new, axis=1, keepdims=True)\n",
    "                return prob, u_new, alpha_new\n",
    "\n",
    "            prob, u, alpha_new = edl12_combine(logits1, logits2)\n",
    "\n",
    "            loss = tf.reduce_mean(mse_loss(tf_train_labels, alpha_new, global_step, annealing_step))\n",
    "            l2_loss = (tf.nn.l2_loss(layer3_weights)+tf.nn.l2_loss(layer4_weights)) * lmb\n",
    "            \n",
    "            if loss_function == None:\n",
    "                total_loss = loss\n",
    "            elif loss_function == 'loss+l2_loss':\n",
    "                total_loss = loss+l2_loss\n",
    "            #optimizer definition\n",
    "            #learning_rate = 0.001\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.9, beta2=0.9, epsilon=1e-08).minimize(total_loss) \n",
    "            \n",
    "            #calculate predictions from training data\n",
    "            train_prediction = prob\n",
    "            #calculate predictions from test data (keep_pr of dropout is 1!)\n",
    "            test_shape = [tf_test_dataset.shape[0], tf_test_dataset.shape[1], tf_test_dataset.shape[2],1]\n",
    "            test_prediction, test_u, _ = edl12_combine(model(tf.reshape(tf_test_dataset[:,:,:,0], test_shape),1),\n",
    "                                                       model(tf.reshape(tf_test_dataset[:,:,:,1], test_shape),1))            \n",
    "            # nuber of iterations\n",
    "            #num_steps = 4001\n",
    "        \n",
    "        #start TensorFlow session\n",
    "        with tf.Session(graph=graph) as session:\n",
    "            tf.global_variables_initializer().run()\n",
    "            print('Initialized')\n",
    "            for step in range(num_steps):\n",
    "                \n",
    "                offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "                \n",
    "                if (offset == 0 ): #if we seen all train data at least once, re-randomize the order of instances\n",
    "                    train_data, train_labels = randomize_data(train_data, train_labels)\n",
    "                \n",
    "                #create batch    \n",
    "                batch_data = train_data[offset:(offset + batch_size), :, :, :]\n",
    "                batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "                \n",
    "                #feed batch data to the placeholders\n",
    "                feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, annealing_step : 10* n_batches }\n",
    "                _, l, predictions = session.run(\n",
    "                    [optimizer, total_loss, train_prediction], feed_dict=feed_dict)\n",
    "               \n",
    "                \n",
    "                # at every 2000. step give some feedback on the progress\n",
    "                if l < 0.0005 or l > 5:\n",
    "                    print('Minibatch loss at step %d: %f' % (step, l))\n",
    "                    print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "                    print('Converged')\n",
    "                    break\n",
    "                if (step % 4000 == 0):\n",
    "                    print('Minibatch loss at step %d: %f' % (step, l))\n",
    "                    print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "\n",
    "            \n",
    "            #evaluate the trained model on the test data in the given fold\n",
    "            test_pred=test_prediction.eval()\n",
    "            print('Test accuracy: %.1f%%' % accuracy(test_pred, test_labels))\n",
    "            \n",
    "            #save test predictions and labels of this fold to a list\n",
    "            test_labs1.append(test_labels)\n",
    "            test_preds1.append(test_pred)\n",
    "\n",
    "    l=test_labs1[0]\n",
    "    p=test_preds1[0]   \n",
    "    #iterate through the cross-validation folds    \n",
    "    for i in range(1,num_folds):\n",
    "        l=np.vstack((l,test_labs1[i]))\n",
    "        p=np.vstack((p,test_preds1[i]))\n",
    "\n",
    "    #calculate final accuracy    \n",
    "    print('ROI EDL12_fuse Test accuracy: %s, %.2f%%' % (name, accuracy(p, l)))\n",
    "    np.savez(\"EDL12_fuse %s.npz\" % name, labels=l,predictions=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 0.721406\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 4000: 0.004560\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.000981\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 10298: 0.000499\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.723119\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 4000: 0.003827\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.000896\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 9855: 0.000497\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.715850\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 4000: 0.004492\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.001104\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 10937: 0.000499\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 50.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.716814\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.004073\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.001041\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 10576: 0.000500\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.721860\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.004805\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.001369\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 11276: 0.000496\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.721041\n",
      "Minibatch accuracy: 25.0%\n",
      "Minibatch loss at step 4000: 0.004951\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.001118\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 10831: 0.000499\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 100.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.712707\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.005385\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.001210\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 10885: 0.000496\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 100.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.722256\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.005055\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.001052\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 10329: 0.000498\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.721842\n",
      "Minibatch accuracy: 75.0%\n",
      "Minibatch loss at step 4000: 0.004334\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.001192\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 11059: 0.000494\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 75.0%\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.721407\n",
      "Minibatch accuracy: 50.0%\n",
      "Minibatch loss at step 4000: 0.003940\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 8000: 0.000919\n",
      "Minibatch accuracy: 100.0%\n",
      "Minibatch loss at step 10365: 0.000497\n",
      "Minibatch accuracy: 100.0%\n",
      "Converged\n",
      "Test accuracy: 75.0%\n",
      "ROI EDL12_fuse Test accuracy: BOLD_combinedDTI, 77.50%\n"
     ]
    }
   ],
   "source": [
    "EDL12_fuse(combined_data, kf, y, name='BOLD_combinedDTI',learning_rate=0.0001, lmb = 0.001, num_steps = 20001, loss_function = 'loss+l2_loss')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1",
   "language": "python",
   "name": "tf1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "80f791d6a2f69e8c2f4231a581aeba9899b74f138f5eec38901bec9a387c2165"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
