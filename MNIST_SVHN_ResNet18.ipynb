{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","from matplotlib import pyplot as plt\n","import scipy.ndimage as nd\n","from tensorflow import keras\n","from keras import backend as K\n","import pdb\n","from tensorflow.keras import layers\n","\n","%matplotlib inline\n","import pylab as pl\n","from IPython import display\n","from sklearn.metrics import accuracy_score\n","import tensorflow_datasets as tfds"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["tf.random.set_seed(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["(mnist_train_images, mnist_train_labels), (mnist_test_images, mnist_test_labels) = tf.keras.datasets.mnist.load_data()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print('MNIST Train Data Shape:', mnist_train_images.shape)\n","print('MNIST Test Data Shape:', mnist_test_images.shape)\n","print('MNIST Train Label Shape:', mnist_train_labels.shape)\n","print('MNIST Test Label Shape:', mnist_test_labels.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from scipy.io import loadmat as load\n","svhn_train = load('/kaggle/input/svhn-mat/train_32x32.mat')\n","svhn_test = load('/kaggle/input/svhn-mat/test_32x32.mat')\n","print('SVHN Train Data Shape:', svhn_train['X'].shape)\n","print('SVHN Test Data Shape:', svhn_test['X'].shape)\n","print('SVHN Train Label Shape:', svhn_train['y'].shape)\n","print('SVHN Test Label Shape:', svhn_test['y'].shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["svhn_train_images = svhn_train['X']\n","svhn_train_labels = svhn_train['y']\n","svhn_test_images = svhn_test['X']\n","svhn_test_labels = svhn_test['y']"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["##svhn labels: 1-10 -> 0-9\n","svhn_train_labels = svhn_train_labels%10\n","svhn_test_labels = svhn_test_labels%10\n","##svhn images (图片高，图片宽，通道数，图片数)->(图片数,图片高，图片宽，通道数)\n","svhn_train_images = np.transpose(svhn_train_images, (3, 0, 1, 2))\n","svhn_test_images = np.transpose(svhn_test_images, (3, 0, 1, 2))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print('SVHN Train Data Shape:', svhn_train_images.shape)\n","print('SVHN Test Data Shape:', svhn_test_images.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","plt.imshow(svhn_train_images[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import torch\n","idx1 = torch.load('/kaggle/input/train-test-idx/train-ms-mnist-idx.pt')\n","idx2 = torch.load('/kaggle/input/train-test-idx/train-ms-svhn-idx.pt')\n","test_idx1 = torch.load('/kaggle/input/train-test-idx/test-ms-mnist-idx.pt')\n","test_idx2 = torch.load('/kaggle/input/train-test-idx/test-ms-svhn-idx.pt')\n","ofd_test_idx1 = torch.load('/kaggle/input/train-test-idx/ofd-test-ms-emnist-idx.pt')\n","ofd_test_idx2 = torch.load('/kaggle/input/train-test-idx/ofd-test-ms-chars74k-idx.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["modal_mnist_samples = mnist_train_images[idx1]\n","modal_svhn_samples = svhn_train_images[idx2]\n","modals_labels = mnist_train_labels[idx1]\n","print(modals_labels.shape)\n","print(len(test_idx1))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["modal_mnist_tests = mnist_test_images[test_idx1]\n","modal_svhn_tests = svhn_test_images[test_idx2]\n","modals_test_labels = mnist_test_labels[test_idx1]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.imshow(modal_mnist_tests[1])\n","plt.imshow(modal_svhn_tests[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def dense_to_one_hot(labels_dense, num_classes=10):\n","    \"\"\"Convert class labels from scalars to one-hot vectors\"\"\"\n","    num_labels = labels_dense.shape[0]\n","    index_offset = np.arange(num_labels) * num_classes\n","    labels_one_hot = np.zeros((num_labels, num_classes))\n","    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n","    return labels_one_hot\n","mnist_train_labels_hot = dense_to_one_hot(mnist_train_labels)\n","mnist_test_labels_hot = dense_to_one_hot(mnist_test_labels)\n","svhn_train_labels_hot = dense_to_one_hot(svhn_train_labels)\n","svhn_test_labels_hot = dense_to_one_hot(svhn_test_labels)\n","modals_labels_hot = dense_to_one_hot(modals_labels)\n","modals_test_labels_hot = dense_to_one_hot(modals_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["k=10\n","def KL(alpha):\n","    beta=tf.constant(np.ones((1,k)),dtype=tf.float32)\n","    S_alpha = tf.reduce_sum(alpha,axis=1,keepdims=True)\n","    S_beta = tf.reduce_sum(beta,axis=1,keepdims=True)\n","    lnB = tf.math.lgamma(S_alpha) - tf.reduce_sum(tf.math.lgamma(alpha),axis=1,keepdims=True)\n","    lnB_uni = tf.reduce_sum(tf.math.lgamma(beta),axis=1,keepdims=True) - tf.math.lgamma(S_beta)\n","    \n","    dg0 = tf.math.digamma(S_alpha)\n","    dg1 = tf.math.digamma(alpha)\n","    \n","    kl = tf.reduce_sum((alpha - beta)*(dg1-dg0),axis=1,keepdims=True) + lnB + lnB_uni\n","    print(kl)\n","    return kl\n","class CustomMSE(keras.losses.Loss):\n","    def __init__(self, annealing_coef=tf.Variable(initial_value=0.0,name='annealing_coef', trainable=False), name=\"custom_mse\"):\n","      super().__init__(name=name)\n","      self.annealing_coef = annealing_coef\n","      self.A = 0\n","      self.B = 0\n","      self.C = 0\n","      #print('Inital:', self.annealing_coef)\n","    def call(self, y_true, y_pred):\n","      y_pred = tf.cast(y_pred, tf.float32)\n","      y_true = tf.cast(y_true, tf.float32)\n","      S = tf.reduce_sum(y_pred, axis=1, keepdims=True) \n","      E = y_pred - 1\n","      m = y_pred / S\n","      #print('y_true:',y_true)\n","      A = tf.reduce_sum((y_true-m)**2, axis=1, keepdims=True) \n","      B = tf.reduce_sum(y_pred*(S-y_pred)/(S*S*(S+1)), axis=1, keepdims=True)       \n","      alp = E*(1-y_true) + 1 \n","      #print('Step:',self.annealing_coef)\n","      C =  self.annealing_coef * KL(alp)\n","      self.A = A\n","      self.B = B\n","      self.C = C\n","      print(A,B,C)\n","      return tf.reduce_mean((A + B) + C)\n","class WarmUp(keras.callbacks.Callback):\n","    \"\"\"\n","    继承Callback，实现对annealing_rate的调度\n","    \"\"\"\n","    def __init__(self,total_steps,global_step_init=0,ac=0,\n","                 verbose=1):\n","        super(WarmUp, self).__init__()\n","        self.total_steps = total_steps\n","        self.global_step = global_step_init\n","        self.ac = ac\n","        self.verbose = verbose\n","        #learning_rates用于记录每次更新后的学习率，方便图形化观察\n","        self.acs = []\n","\t#更新global_step，并记录当前退火系数\n","    def on_batch_end(self, batch, logs=None):\n","        self.global_step = self.global_step + 1\n","        ac = K.get_value(self.model.loss.annealing_coef)\n","        self.acs.append(ac)\n","\t#更新退火系数\n","    def on_batch_begin(self, batch, logs=None):\n","        ac = tf.minimum(1.0,tf.cast(self.global_step/self.total_steps,tf.float32)).numpy()\n","        #print('ac:',ac)\n","        K.set_value(self.model.loss.annealing_coef, ac)\n","        #print('after set',self.model.loss.annealing_coef)\n","        if self.verbose > 0:\n","          print('\\nBatch %05d: setting annealing_coef '\n","                  'rate to %s.' % (self.global_step + 1, ac))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def conv2d_bn(inpt, filters=64, kernel_size=(3,3), strides=1, padding='same'):\n","    '''卷积、归一化和relu三合一'''\n","    x = layers.Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding=padding)(inpt)\n","    x = layers.BatchNormalization()(x)\n","    x = tf.keras.layers.Activation('relu')(x)\n","    return x\n","\n","def basic_bottle(inpt, filters=64, kernel_size=(3,3), strides=1, padding='same', if_baisc=False):\n","    '''18中的4个basic_bottle'''\n","    x = conv2d_bn(inpt, filters=filters, kernel_size=kernel_size, strides=strides, padding=padding)\n","    x = conv2d_bn(x, filters=filters)\n","    if if_baisc==True:\n","        temp = conv2d_bn(inpt, filters=filters, kernel_size=(1,1), strides=2, padding='same')\n","        outt = layers.add([x, temp])\n","    else:\n","        outt = layers.add([x, inpt])\n","    return outt\n","\n","def resnet18(class_nums,input_shape=(28,28,1)):\n","    '''主模型'''\n","    inpt = layers.Input(shape=input_shape)\n","    #layer 1\n","    x = conv2d_bn(inpt, filters=64, kernel_size=(7,7), strides=2, padding='valid')\n","    x = layers.MaxPool2D(pool_size=(3,3), strides=2)(x)\n","    #layer 2\n","    x = basic_bottle(x, filters=64, kernel_size=(3,3), strides=1, padding='same', if_baisc=False)\n","    x = basic_bottle(x, filters=64, kernel_size=(3,3), strides=1, padding='same', if_baisc=False)\n","    #layer 3\n","    x = basic_bottle(x, filters=128, kernel_size=(3, 3), strides=2, padding='same', if_baisc=True)\n","    x = basic_bottle(x, filters=128, kernel_size=(3, 3), strides=1, padding='same', if_baisc=False)\n","    # layer 4\n","    x = basic_bottle(x, filters=256, kernel_size=(3, 3), strides=2, padding='same', if_baisc=True)\n","    x = basic_bottle(x, filters=256, kernel_size=(3, 3), strides=1, padding='same', if_baisc=False)\n","    # layer 5\n","    x = basic_bottle(x, filters=512, kernel_size=(3, 3), strides=2, padding='same', if_baisc=True)\n","    x = basic_bottle(x, filters=512, kernel_size=(3, 3), strides=1, padding='same', if_baisc=False)\n","    #GlobalAveragePool\n","    x = layers.GlobalAveragePooling2D()(x)\n","    x = layers.Dense(class_nums, activation='softmax')(x)\n","    model = tf.keras.Model(inputs=inpt, outputs=x)\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_mnist = resnet18(class_nums=10,input_shape=(28,28,1))\n","#model_mnist.summary()\n","model_mnist.compile(optimizer=tf.keras.optimizers.Adam(),loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\n","model_mnist.fit(modal_mnist_samples, modals_labels, batch_size=64, epochs=10) \n","mnist_test_preds = model_mnist.predict(modal_mnist_tests)\n","y_mnist_preds = np.argmax(mnist_test_preds, axis=1)\n","acc_score_mnist = accuracy_score(y_true=modals_test_labels,y_pred=y_mnist_preds)\n","print(acc_score_mnist)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_svhn = resnet18(class_nums=10,input_shape=(32,32,3))\n","#model_svhn.summary()\n","model_svhn.compile(optimizer=tf.keras.optimizers.Adam(),loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\n","model_svhn.fit(modal_svhn_samples, modals_labels, batch_size=64, epochs=10) \n","svhn_test_preds = model_svhn.predict(modal_svhn_tests)\n","y_svhn_preds = np.argmax(svhn_test_preds, axis=1)\n","acc_score_svhn = accuracy_score(y_true=modals_test_labels,y_pred=y_svhn_preds)\n","print(acc_score_svhn)"]},{"cell_type":"markdown","metadata":{},"source":["### Out of distribution testing"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["(emnist_images, emnist_labels) =  tfds.as_numpy(tfds.load('emnist/letters',\n","                         split = 'test', \n","                         batch_size=-1, \n","                         as_supervised=True))\n","chars74k_images = np.load('/kaggle/input/chars74k/EnglishImages.npy')\n","chars74k_labels = np.load('/kaggle/input/chars74k/EnglishLabels.npy')\n","ofd_emnist_tests = emnist_images[ofd_test_idx1]\n","ofd_chars74k_tests = chars74k_images[ofd_test_idx2]\n","ofd_test_labels = emnist_labels[ofd_test_idx1]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["mnist_ofd_test_preds = model_mnist.predict(ofd_emnist_tests)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def resnet18EDL1(class_nums,input_shape=(28,28,1)):\n","    '''主模型'''\n","    inpt = layers.Input(shape=input_shape)\n","    #layer 1\n","    x = conv2d_bn(inpt, filters=64, kernel_size=(7,7), strides=2, padding='valid')\n","    x = layers.MaxPool2D(pool_size=(3,3), strides=2)(x)\n","    #layer 2\n","    x = basic_bottle(x, filters=64, kernel_size=(3,3), strides=1, padding='same', if_baisc=False)\n","    x = basic_bottle(x, filters=64, kernel_size=(3,3), strides=1, padding='same', if_baisc=False)\n","    #layer 3\n","    x = basic_bottle(x, filters=128, kernel_size=(3, 3), strides=2, padding='same', if_baisc=True)\n","    x = basic_bottle(x, filters=128, kernel_size=(3, 3), strides=1, padding='same', if_baisc=False)\n","    # layer 4\n","    x = basic_bottle(x, filters=256, kernel_size=(3, 3), strides=2, padding='same', if_baisc=True)\n","    x = basic_bottle(x, filters=256, kernel_size=(3, 3), strides=1, padding='same', if_baisc=False)\n","    # layer 5\n","    x = basic_bottle(x, filters=512, kernel_size=(3, 3), strides=2, padding='same', if_baisc=True)\n","    x = basic_bottle(x, filters=512, kernel_size=(3, 3), strides=1, padding='same', if_baisc=False)\n","    #GlobalAveragePool\n","    x = layers.GlobalAveragePooling2D()(x)\n","    e = layers.Dense(class_nums, activation='relu')(x)\n","    b = layers.Dense(class_nums, activation='softmax')(x)\n","    E = tf.reduce_sum(e, axis=1, keepdims=True)\n","    alpha = E * b+1\n","    model = tf.keras.Model(inputs=inpt, outputs=alpha)\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_mnist_edl1 = resnet18EDL1(class_nums=10,input_shape=(28,28,1))\n","#model_mnist.summary()\n","annealing_step = 10*(len(modals_labels)//64)\n","model_mnist_edl1.compile(optimizer=tf.keras.optimizers.Adam(),loss=CustomMSE(), metrics=['accuracy'])\n","warm_up_ac = WarmUp(total_steps=annealing_step,global_step_init=0,ac=0,verbose=0)\n","history_edl_mnist1=model_mnist_edl1.fit(modal_mnist_samples, modals_labels_hot, batch_size=64, epochs=10,verbose=1,callbacks=[warm_up_ac])\n","mnist_test_preds1 = model_mnist_edl1.predict(modal_mnist_tests)\n","y_mnist_preds1 = np.argmax(mnist_test_preds1, axis=1)\n","acc_score_mnist1 = accuracy_score(y_true=modals_test_labels,y_pred=y_mnist_preds1)\n","print(acc_score_mnist1)\n","\n","model_svhn_edl1 = resnet18EDL1(class_nums=10,input_shape=(32,32,3))\n","#model_mnist.summary()\n","annealing_step = 10*(len(modals_labels)//64)\n","model_svhn_edl1.compile(optimizer=tf.keras.optimizers.Adam(),loss=CustomMSE(), metrics=['accuracy'])\n","warm_up_ac = WarmUp(total_steps=annealing_step,global_step_init=0,ac=0,verbose=0)\n","model_svhn_edl1.fit(modal_svhn_samples, modals_labels_hot, batch_size=64, epochs=10,verbose=1,callbacks=[warm_up_ac])\n","svhn_test_preds1 = model_svhn_edl1.predict(modal_svhn_tests)\n","y_svhn_preds1 = np.argmax(svhn_test_preds1, axis=1)\n","acc_score_svhn1 = accuracy_score(y_true=modals_test_labels,y_pred=y_svhn_preds1)\n","print(acc_score_svhn1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["edl1_mnist_ofd_test_preds = model_mnist_edl1.predict(ofd_emnist_tests)\n","edl1_mnist_ents = st.entropy(edl1_mnist_ofd_test_preds,axis=1)\n","print(edl1_mnist_ents.mean())\n","sns.displot(edl1_mnist_ents, kde=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def resnet18EDL1_fuse(class_nums,input_shape1=(28,28,1), input_shape2=(32,32,3)):\n","    '''主模型'''\n","    inpt1 = layers.Input(shape=input_shape1)\n","    inpt2 = layers.Input(shape=input_shape2)\n","    #layer 1\n","    x1 = conv2d_bn(inpt1, filters=64, kernel_size=(7,7), strides=2, padding='valid')\n","    x1 = layers.MaxPool2D(pool_size=(3,3), strides=2)(x1)\n","    #layer 2\n","    x1 = basic_bottle(x1, filters=64, kernel_size=(3,3), strides=1, padding='same', if_baisc=False)\n","    x1 = basic_bottle(x1, filters=64, kernel_size=(3,3), strides=1, padding='same', if_baisc=False)\n","    #layer 3\n","    x1 = basic_bottle(x1, filters=128, kernel_size=(3, 3), strides=2, padding='same', if_baisc=True)\n","    x1 = basic_bottle(x1, filters=128, kernel_size=(3, 3), strides=1, padding='same', if_baisc=False)\n","    # layer 4\n","    x1 = basic_bottle(x1, filters=256, kernel_size=(3, 3), strides=2, padding='same', if_baisc=True)\n","    x1 = basic_bottle(x1, filters=256, kernel_size=(3, 3), strides=1, padding='same', if_baisc=False)\n","    # layer 5\n","    x1 = basic_bottle(x1, filters=512, kernel_size=(3, 3), strides=2, padding='same', if_baisc=True)\n","    x1 = basic_bottle(x1, filters=512, kernel_size=(3, 3), strides=1, padding='same', if_baisc=False)\n","    #GlobalAveragePool\n","    x1 = layers.GlobalAveragePooling2D()(x1)\n","    e1 = layers.Dense(class_nums, activation='relu')(x1)\n","    b1 = layers.Dense(class_nums, activation='softmax')(x1) \n","\n","    alpha1 = e1# + 1\n","    S1 = tf.reduce_sum(alpha1, axis=1, keepdims=True)\n","    #E1 = tf.reduce_sum(e1, axis=1, keepdims=True)\n","    #new_e1 = E1*b1\n","    #alpha1 = new_e1 + 1\n","    #S1 = tf.reduce_sum(alpha1, axis=1, keepdims=True)\n","    #new_b1 = new_e1/S1\n","    #u1 = class_nums/S1\n","\n","    #layer 1\n","    x2 = conv2d_bn(inpt2, filters=64, kernel_size=(7,7), strides=2, padding='valid')\n","    x2 = layers.MaxPool2D(pool_size=(3,3), strides=2)(x2)\n","    #layer 2\n","    x2 = basic_bottle(x2, filters=64, kernel_size=(3,3), strides=1, padding='same', if_baisc=False)\n","    x2 = basic_bottle(x2, filters=64, kernel_size=(3,3), strides=1, padding='same', if_baisc=False)\n","    #layer 3\n","    x2 = basic_bottle(x2, filters=128, kernel_size=(3, 3), strides=2, padding='same', if_baisc=True)\n","    x2 = basic_bottle(x2, filters=128, kernel_size=(3, 3), strides=1, padding='same', if_baisc=False)\n","    # layer 4\n","    x2 = basic_bottle(x2, filters=256, kernel_size=(3, 3), strides=2, padding='same', if_baisc=True)\n","    x2 = basic_bottle(x2, filters=256, kernel_size=(3, 3), strides=1, padding='same', if_baisc=False)\n","    # layer 5\n","    x2 = basic_bottle(x2, filters=512, kernel_size=(3, 3), strides=2, padding='same', if_baisc=True)\n","    x2 = basic_bottle(x2, filters=512, kernel_size=(3, 3), strides=1, padding='same', if_baisc=False)\n","    #GlobalAveragePool\n","    x2 = layers.GlobalAveragePooling2D()(x2)\n","    e2 = layers.Dense(class_nums, activation='relu')(x2)\n","    b2 = layers.Dense(class_nums, activation='softmax')(x2)\n","\n","    #x2 = layers.Dense(class_nums, activation='linear')(x2)\n","    #e2 = layers.Activation('relu')(x2)\n","    #b2 = layers.Activation('softmax')(x2)\n","\n","    alpha2 = e2# + 1\n","    S2 = tf.reduce_sum(alpha2, axis=1, keepdims=True)\n","    #E2 = tf.reduce_sum(e2, axis=1, keepdims=True)\n","    #new_e2 = E2*b2\n","    #alpha2 = new_e2 + 1\n","    #S2 = tf.reduce_sum(alpha2, axis=1, keepdims=True)\n","    #new_b2 = new_e2/S2\n","    #u2 = class_nums/S2\n","    \n","    S = (S1 + S2) /2\n","    #b = new_b1*new_b2 + u1*new_b2 + u2*new_b1 \n","    b = b1*b2\n","    kappa = tf.reduce_sum(b,axis=1,keepdims=True)\n","    b_new = b/kappa\n","    alpha_new = S*b_new+1\n","\n","    model = tf.keras.Model(inputs=(inpt1,inpt2), outputs=alpha_new)\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def resnet18EDL2(class_nums,input_shape=(28,28,1)):\n","    '''主模型'''\n","    inpt = layers.Input(shape=input_shape)\n","    #layer 1\n","    x = conv2d_bn(inpt, filters=64, kernel_size=(7,7), strides=2, padding='valid')\n","    x = layers.MaxPool2D(pool_size=(3,3), strides=2)(x)\n","    #layer 2\n","    x = basic_bottle(x, filters=64, kernel_size=(3,3), strides=1, padding='same', if_baisc=False)\n","    x = basic_bottle(x, filters=64, kernel_size=(3,3), strides=1, padding='same', if_baisc=False)\n","    #layer 3\n","    x = basic_bottle(x, filters=128, kernel_size=(3, 3), strides=2, padding='same', if_baisc=True)\n","    x = basic_bottle(x, filters=128, kernel_size=(3, 3), strides=1, padding='same', if_baisc=False)\n","    # layer 4\n","    x = basic_bottle(x, filters=256, kernel_size=(3, 3), strides=2, padding='same', if_baisc=True)\n","    x = basic_bottle(x, filters=256, kernel_size=(3, 3), strides=1, padding='same', if_baisc=False)\n","    # layer 5\n","    x = basic_bottle(x, filters=512, kernel_size=(3, 3), strides=2, padding='same', if_baisc=True)\n","    x = basic_bottle(x, filters=512, kernel_size=(3, 3), strides=1, padding='same', if_baisc=False)\n","    #GlobalAveragePool\n","    x = layers.GlobalAveragePooling2D()(x)\n","    e = layers.Dense(class_nums, activation='relu')(x)\n","    alpha = e + 1\n"," \n","    model = tf.keras.Model(inputs=inpt, outputs=alpha)\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_mnist_edl2 = resnet18EDL2(class_nums=10,input_shape=(28,28,1))\n","#model_mnist.summary()\n","annealing_step = 10*(len(modals_labels)//64)\n","model_mnist_edl2.compile(optimizer=tf.keras.optimizers.Adam(),loss=CustomMSE(), metrics=['accuracy'])\n","warm_up_ac = WarmUp(total_steps=annealing_step,global_step_init=0,ac=0,verbose=0)\n","history_edl_mnist2=model_mnist_edl2.fit(modal_mnist_samples, modals_labels_hot, batch_size=64, epochs=10,verbose=1,callbacks=[warm_up_ac])\n","mnist_test_preds2 = model_mnist_edl2.predict(modal_mnist_tests)\n","y_mnist_preds2 = np.argmax(mnist_test_preds2, axis=1)\n","acc_score_mnist2 = accuracy_score(y_true=modals_test_labels,y_pred=y_mnist_preds2)\n","print(acc_score_mnist2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_svhn_edl2 = resnet18EDL2(class_nums=10,input_shape=(32,32,3))\n","#model_mnist.summary()\n","annealing_step = 10*(len(modals_labels)//64)\n","model_svhn_edl2.compile(optimizer=tf.keras.optimizers.Adam(),loss=CustomMSE(), metrics=['accuracy'])\n","warm_up_ac = WarmUp(total_steps=annealing_step,global_step_init=0,ac=0,verbose=0)\n","model_svhn_edl2.fit(modal_svhn_samples, modals_labels_hot, batch_size=64, epochs=10,verbose=1,callbacks=[warm_up_ac])\n","svhn_test_preds2 = model_svhn_edl2.predict(modal_svhn_tests)\n","y_svhn_preds2 = np.argmax(svhn_test_preds2, axis=1)\n","acc_score_svhn2 = accuracy_score(y_true=modals_test_labels,y_pred=y_svhn_preds2)\n","print(acc_score_svhn2)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["edl2_mnist_ofd_test_preds = model_mnist_edl2.predict(ofd_emnist_tests)\n","edl2_mnist_ents = st.entropy(edl2_mnist_ofd_test_preds,axis=1)\n","print(edl2_mnist_ents.mean())\n","sns.displot(edl2_mnist_ents, kde=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def resnet18EDL2_fuse(class_nums,input_shape1=(28,28,1), input_shape2=(32,32,3)):\n","    '''主模型'''\n","    inpt1 = layers.Input(shape=input_shape1)\n","    inpt2 = layers.Input(shape=input_shape2)\n","    #layer 1\n","    x1 = conv2d_bn(inpt1, filters=64, kernel_size=(7,7), strides=2, padding='valid')\n","    x1 = layers.MaxPool2D(pool_size=(3,3), strides=2)(x1)\n","    #layer 2\n","    x1 = basic_bottle(x1, filters=64, kernel_size=(3,3), strides=1, padding='same', if_baisc=False)\n","    x1 = basic_bottle(x1, filters=64, kernel_size=(3,3), strides=1, padding='same', if_baisc=False)\n","    #layer 3\n","    x1 = basic_bottle(x1, filters=128, kernel_size=(3, 3), strides=2, padding='same', if_baisc=True)\n","    x1 = basic_bottle(x1, filters=128, kernel_size=(3, 3), strides=1, padding='same', if_baisc=False)\n","    # layer 4\n","    x1 = basic_bottle(x1, filters=256, kernel_size=(3, 3), strides=2, padding='same', if_baisc=True)\n","    x1 = basic_bottle(x1, filters=256, kernel_size=(3, 3), strides=1, padding='same', if_baisc=False)\n","    # layer 5\n","    x1 = basic_bottle(x1, filters=512, kernel_size=(3, 3), strides=2, padding='same', if_baisc=True)\n","    x1 = basic_bottle(x1, filters=512, kernel_size=(3, 3), strides=1, padding='same', if_baisc=False)\n","    #GlobalAveragePool\n","    x1 = layers.GlobalAveragePooling2D()(x1)\n","    e1 = layers.Dense(class_nums, activation='relu')(x1) \n","    alpha1 = e1 + 1\n","    S1 = tf.reduce_sum(alpha1, axis=1, keepdims=True)\n","    u1 = class_nums/S1\n","    b1 = e1/S1\n","\n","    #layer 1\n","    x2 = conv2d_bn(inpt2, filters=64, kernel_size=(7,7), strides=2, padding='valid')\n","    x2 = layers.MaxPool2D(pool_size=(3,3), strides=2)(x2)\n","    #layer 2\n","    x2 = basic_bottle(x2, filters=64, kernel_size=(3,3), strides=1, padding='same', if_baisc=False)\n","    x2 = basic_bottle(x2, filters=64, kernel_size=(3,3), strides=1, padding='same', if_baisc=False)\n","    #layer 3\n","    x2 = basic_bottle(x2, filters=128, kernel_size=(3, 3), strides=2, padding='same', if_baisc=True)\n","    x2 = basic_bottle(x2, filters=128, kernel_size=(3, 3), strides=1, padding='same', if_baisc=False)\n","    # layer 4\n","    x2 = basic_bottle(x2, filters=256, kernel_size=(3, 3), strides=2, padding='same', if_baisc=True)\n","    x2 = basic_bottle(x2, filters=256, kernel_size=(3, 3), strides=1, padding='same', if_baisc=False)\n","    # layer 5\n","    x2 = basic_bottle(x2, filters=512, kernel_size=(3, 3), strides=2, padding='same', if_baisc=True)\n","    x2 = basic_bottle(x2, filters=512, kernel_size=(3, 3), strides=1, padding='same', if_baisc=False)\n","    #GlobalAveragePool\n","    x2 = layers.GlobalAveragePooling2D()(x2)\n","    e2 = layers.Dense(class_nums, activation='relu')(x2)\n","    \n","    alpha2 = e2 + 1\n","    S2 = tf.reduce_sum(alpha2, axis=1, keepdims=True)\n","    u2 = class_nums/S2\n","    b2 = e2/S2\n","\n","    #b = new_b1*new_b2 + u1*new_b2 + u2*new_b1 \n","    b = b1*b2 + b1*u2 + b2*u1\n","    u = u1*u2\n","    kappa = tf.reduce_sum(b,axis=1,keepdims=True)+u\n","    b_new = b/kappa\n","    u_new = u/kappa\n","    S = class_nums/u_new\n","    alpha_new = S*b_new+1\n","    model = tf.keras.Model(inputs=(inpt1,inpt2), outputs=alpha_new)\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_edl2_fuse = resnet18EDL2_fuse(class_nums=10,input_shape1=(28,28,1), input_shape2=(32,32,3))\n","#model_mnist.summary()\n","annealing_step = 10*(len(modals_labels)//64)\n","model_edl2_fuse.compile(optimizer=tf.keras.optimizers.Adam(),loss=CustomMSE(), metrics=['accuracy'])\n","warm_up_ac = WarmUp(total_steps=annealing_step,global_step_init=0,ac=0,verbose=0)\n","history_edl2_fuse=model_edl2_fuse.fit((modal_mnist_samples,modal_svhn_samples), modals_labels_hot, batch_size=64, epochs=10,verbose=1,callbacks=[warm_up_ac])\n","test_preds_fuse2 = model_edl2_fuse.predict((modal_mnist_tests,modal_svhn_tests))\n","y_preds_fuse2 = np.argmax(test_preds_fuse2, axis=1)\n","acc_score_fuse2 = accuracy_score(y_true=modals_test_labels,y_pred=y_preds_fuse2)\n","print(acc_score_fuse2)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def resnet18EDL12(class_nums,input_shape=(28,28,1)):\n","    inpt = layers.Input(shape=input_shape)\n","    #layer 1\n","    x = conv2d_bn(inpt, filters=64, kernel_size=(7,7), strides=2, padding='valid')\n","    x = layers.MaxPool2D(pool_size=(3,3), strides=2)(x)\n","    #layer 2\n","    x = basic_bottle(x, filters=64, kernel_size=(3,3), strides=1, padding='same', if_baisc=False)\n","    x = basic_bottle(x, filters=64, kernel_size=(3,3), strides=1, padding='same', if_baisc=False)\n","    #layer 3\n","    x = basic_bottle(x, filters=128, kernel_size=(3, 3), strides=2, padding='same', if_baisc=True)\n","    x = basic_bottle(x, filters=128, kernel_size=(3, 3), strides=1, padding='same', if_baisc=False)\n","    # layer 4\n","    x = basic_bottle(x, filters=256, kernel_size=(3, 3), strides=2, padding='same', if_baisc=True)\n","    x = basic_bottle(x, filters=256, kernel_size=(3, 3), strides=1, padding='same', if_baisc=False)\n","    # layer 5\n","    x = basic_bottle(x, filters=512, kernel_size=(3, 3), strides=2, padding='same', if_baisc=True)\n","    x = basic_bottle(x, filters=512, kernel_size=(3, 3), strides=1, padding='same', if_baisc=False)\n","    #GlobalAveragePool\n","    x = layers.GlobalAveragePooling2D()(x)\n","    e = layers.Dense(class_nums, activation='relu')(x)\n","    b2 = layers.Dense(class_nums, activation='softmax')(x)\n","\n","    alpha1 = e + 1\n","    S = tf.reduce_sum(alpha1,axis=1,keepdims=True)\n","    b1 = e / S\n","    u1 = class_nums/S\n","    b = b1*b2+u1*b2\n","\n","    kappa = tf.reduce_sum(b,axis=1,keepdims=True)\n","    b_new = (1-u1)*b/kappa\n","    alpha_new = S*b_new+1\n","    print(alpha_new) \n","    model = tf.keras.Model(inputs=inpt, outputs=alpha_new)\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_mnist_edl12 = resnet18EDL12(class_nums=10,input_shape=(28,28,1))\n","#model_mnist.summary()\n","annealing_step = 10*(len(modals_labels)//64)\n","model_mnist_edl12.compile(optimizer=tf.keras.optimizers.Adam(),loss=CustomMSE(), metrics=['accuracy'])\n","warm_up_ac = WarmUp(total_steps=annealing_step,global_step_init=0,ac=0,verbose=0)\n","history_edl_mnist12=model_mnist_edl12.fit(modal_mnist_samples, modals_labels_hot, batch_size=64, epochs=10,verbose=1,callbacks=[warm_up_ac])\n","mnist_test_preds12 = model_mnist_edl12.predict(modal_mnist_tests)\n","y_mnist_preds12 = np.argmax(mnist_test_preds12, axis=1)\n","acc_score_mnist12 = accuracy_score(y_true=modals_test_labels,y_pred=y_mnist_preds12)\n","print(acc_score_mnist12)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def resnet18EDL12_fuse(class_nums,input_shape1=(28,28,1), input_shape2=(32,32,3)):\n","    '''主模型'''\n","    inpt1 = layers.Input(shape=input_shape1)\n","    inpt2 = layers.Input(shape=input_shape2)\n","    #layer 1\n","    x1 = conv2d_bn(inpt1, filters=64, kernel_size=(7,7), strides=2, padding='valid')\n","    x1 = layers.MaxPool2D(pool_size=(3,3), strides=2)(x1)\n","    #layer 2\n","    x1 = basic_bottle(x1, filters=64, kernel_size=(3,3), strides=1, padding='same', if_baisc=False)\n","    x1 = basic_bottle(x1, filters=64, kernel_size=(3,3), strides=1, padding='same', if_baisc=False)\n","    #layer 3\n","    x1 = basic_bottle(x1, filters=128, kernel_size=(3, 3), strides=2, padding='same', if_baisc=True)\n","    x1 = basic_bottle(x1, filters=128, kernel_size=(3, 3), strides=1, padding='same', if_baisc=False)\n","    # layer 4\n","    x1 = basic_bottle(x1, filters=256, kernel_size=(3, 3), strides=2, padding='same', if_baisc=True)\n","    x1 = basic_bottle(x1, filters=256, kernel_size=(3, 3), strides=1, padding='same', if_baisc=False)\n","    # layer 5\n","    x1 = basic_bottle(x1, filters=512, kernel_size=(3, 3), strides=2, padding='same', if_baisc=True)\n","    x1 = basic_bottle(x1, filters=512, kernel_size=(3, 3), strides=1, padding='same', if_baisc=False)\n","    #GlobalAveragePool\n","    x1 = layers.GlobalAveragePooling2D()(x1)\n","    e1 = layers.Dense(class_nums, activation='relu')(x1)\n","    b12 = layers.Dense(class_nums, activation='softmax')(x1)\n","\n","    alpha11 = e1 + 1\n","    S1 = tf.reduce_sum(alpha11,axis=1,keepdims=True)\n","    b11 = e1 / S1\n","    u1 = class_nums/S1\n","    b1 = b11*b12+u1*b12\n","    kappa1 = tf.reduce_sum(b1,axis=1,keepdims=True)\n","    b_new1 = (1-u1)*b1/kappa1\n","    alpha_new1 = S1*b_new1+1\n","\n","    #layer 1\n","    x2 = conv2d_bn(inpt2, filters=64, kernel_size=(7,7), strides=2, padding='valid')\n","    x2 = layers.MaxPool2D(pool_size=(3,3), strides=2)(x2)\n","    #layer 2\n","    x2 = basic_bottle(x2, filters=64, kernel_size=(3,3), strides=1, padding='same', if_baisc=False)\n","    x2 = basic_bottle(x2, filters=64, kernel_size=(3,3), strides=1, padding='same', if_baisc=False)\n","    #layer 3\n","    x2 = basic_bottle(x2, filters=128, kernel_size=(3, 3), strides=2, padding='same', if_baisc=True)\n","    x2 = basic_bottle(x2, filters=128, kernel_size=(3, 3), strides=1, padding='same', if_baisc=False)\n","    # layer 4\n","    x2 = basic_bottle(x2, filters=256, kernel_size=(3, 3), strides=2, padding='same', if_baisc=True)\n","    x2 = basic_bottle(x2, filters=256, kernel_size=(3, 3), strides=1, padding='same', if_baisc=False)\n","    # layer 5\n","    x2 = basic_bottle(x2, filters=512, kernel_size=(3, 3), strides=2, padding='same', if_baisc=True)\n","    x2 = basic_bottle(x2, filters=512, kernel_size=(3, 3), strides=1, padding='same', if_baisc=False)\n","    #GlobalAveragePool\n","    x2 = layers.GlobalAveragePooling2D()(x2)\n","    e2 = layers.Dense(class_nums, activation='relu')(x2)\n","    b22 = layers.Dense(class_nums, activation='softmax')(x2)\n","\n","    alpha21 = e2 + 1\n","    S2 = tf.reduce_sum(alpha21,axis=1,keepdims=True)\n","    b21 = e2 / S2\n","    u2 = class_nums/S2\n","    b2 = b21*b22+u2*b22\n","    kappa2 = tf.reduce_sum(b2,axis=1,keepdims=True)\n","    b_new2 = (1-u2)*b2/kappa2\n","    alpha_new2 = S2*b_new2+1\n","\n","    #b = new_b1*new_b2 + u1*new_b2 + u2*new_b1 \n","    b = b_new1*b_new2 + b_new1*u2 + b_new2*u1\n","    u = u1*u2\n","    kappa = tf.reduce_sum(b,axis=1,keepdims=True)+u\n","    b_new = b/kappa\n","    u_new = u/kappa\n","    S = class_nums/u_new\n","    alpha_new = S*b_new+1\n","    model = tf.keras.Model(inputs=(inpt1,inpt2), outputs=alpha_new)\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_edl12_fuse = resnet18EDL12_fuse(class_nums=10,input_shape1=(28,28,1), input_shape2=(32,32,3))\n","#model_mnist.summary()\n","annealing_step = 10*(len(modals_labels)//64)\n","model_edl12_fuse.compile(optimizer=tf.keras.optimizers.Adam(),loss=CustomMSE(), metrics=['accuracy'])\n","warm_up_ac = WarmUp(total_steps=annealing_step,global_step_init=0,ac=0,verbose=0)\n","history_edl12_fuse=model_edl12_fuse.fit((modal_mnist_samples,modal_svhn_samples), modals_labels_hot, batch_size=64, epochs=10,verbose=1,callbacks=[warm_up_ac])\n","test_preds_fuse12 = model_edl12_fuse.predict((modal_mnist_tests,modal_svhn_tests))\n","y_preds_fuse12 = np.argmax(test_preds_fuse12, axis=1)\n","acc_score_fuse12 = accuracy_score(y_true=modals_test_labels,y_pred=y_preds_fuse12)\n","print(acc_score_fuse12)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_edl12_fuse = resnet18EDL12_fuse(class_nums=10,input_shape1=(28,28,1), input_shape2=(32,32,3))\n","#model_mnist.summary()\n","annealing_step = 10*(len(modals_labels)//64)\n","model_edl12_fuse.compile(optimizer=tf.keras.optimizers.Adam(),loss=CustomMSE(), metrics=['accuracy'])\n","warm_up_ac = WarmUp(total_steps=annealing_step,global_step_init=0,ac=0,verbose=0)\n","history_edl12_fuse=model_edl12_fuse.fit((modal_mnist_samples,modal_svhn_samples), modals_labels_hot, batch_size=64, epochs=10,verbose=1,callbacks=[warm_up_ac])\n","test_preds_fuse12 = model_edl12_fuse.predict((modal_mnist_tests,modal_svhn_tests))\n","y_preds_fuse12 = np.argmax(test_preds_fuse12, axis=1)\n","acc_score_fuse12 = accuracy_score(y_true=modals_test_labels,y_pred=y_preds_fuse12)\n","print(acc_score_fuse12)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_preds_fuse12 = model_edl12_fuse.predict((modal_mnist_tests,modal_svhn_tests))\n","y_preds_fuse12 = np.argmax(test_preds_fuse12, axis=1)\n","acc_score_fuse12 = accuracy_score(y_true=modals_test_labels,y_pred=y_preds_fuse12)\n","print(acc_score_fuse12)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
